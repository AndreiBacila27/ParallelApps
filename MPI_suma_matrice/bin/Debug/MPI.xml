<?xml version="1.0"?>
<doc>
    <assembly>
        <name>MPI</name>
    </assembly>
    <members>
        <member name="T:MPI.TagAllocator">
            <summary>
            Allocates unique tags for the various point-to-point communications that are used within 
            the MPI.NET implementation but must remain hidden from the user. All of the routines
            in the <c>TagAllocator</c> class are thread-safe when MPI is running in a mode that supports
            multi-threading.
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.#ctor">
            <summary>
            Initialize a new tag allocator
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.AllocateSendTag">
            <summary>
            Allocate a new, unique tag for a single serialized send. The tag
            must be returned via <see cref="M:MPI.TagAllocator.ReturnSendTag(System.Int32)"/>.
            </summary>
            <returns>A new, unique tag for use with this serialized send.</returns>
        </member>
        <member name="M:MPI.TagAllocator.AllocateSendTagImpl">
            <summary>
            Actual implementation of <see cref="M:MPI.TagAllocator.AllocateSendTag"/>.
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.ReturnSendTag(System.Int32)">
            <summary>
            Returns a tag allocated via <see cref="M:MPI.TagAllocator.AllocateSendTag"/>.
            </summary>
            <param name="tag">The tag to return.</param>
        </member>
        <member name="M:MPI.TagAllocator.ReturnSendTagImpl(System.Int32)">
            <summary>
            Implementation of <see cref="M:MPI.TagAllocator.ReturnSendTag(System.Int32)"/>.
            </summary>
            <param name="tag">Tag value being returned.</param>
        </member>
        <member name="F:MPI.TagAllocator.nextSendTag">
            <summary>
            The next unique tag that will be given out for a serialized send. This
            value starts at zero and increases as more tags are requested.
            </summary>
        </member>
        <member name="F:MPI.TagAllocator.returnedSendTags">
            <summary>
            Stack of send tags that have been returned out-of-order. We always
            exhaust these tags first, before allocating a new send tag by increasing
            <see cref="F:MPI.TagAllocator.nextSendTag"/>.
            </summary>
        </member>
        <member name="T:MPI.Intercommunicator">
            <summary>
              Intercommunicators are <see cref="T:MPI.Communicator"/>s that contain two disjoint groups of 
              processes, call them A and B. Any process in group A can send
              a message to or receive a message from any process in group B, and vice-versa. However, there
              is no way to use an intercommunicator to send messages among the processes within a group.
              Intercommunicators are often useful in large MPI applications that tie together many, smaller
              modules. Typically, each module will have its own intracommunicators and the modules will 
              interact with each other via intercommunicators.
            </summary>
        </member>
        <member name="T:MPI.Communicator">
            <summary>
              The communicator class abstracts a set of communicating processes in MPI.
              Communicators are the most fundamental types in MPI, because they are the basis
              of all inter-process communication. Each communicator provides a separate communication
              space for a set of processes, so that the messages transmitted with one communicator 
              will not collide with messages transmitted with another communicator. As such, different
              libraries and different tasks can all use MPI without colliding, so long as they are using
              different communicators. There are two important kinds of communicators: intracommunicators 
              and intercommunicators.
            
              <see cref="T:MPI.Intracommunicator"/>s are the most commonly used form of communicator. 
              Each intracommunicator contains a set of processes, each of which is identified by its
              "rank" within the communicator. The ranks are numbered 0 through <see cref="P:MPI.Communicator.Size"/>-1. 
              Any process in the communicator can <see cref="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)"/> a message 
              to another process
              within the communicator or <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/> a message from any other process in 
              the communicator. Intracommunicators also support a variety of collective operations
              that involve all of the processes in the communicator. Most MPI communication occurs
              within intracommunicators, with very few MPI programs requiring intercommunicators.
            
              <see cref="T:MPI.Intercommunicator"/>s differ from intracommunicators in that intercommunicators
              contain two disjoint groups of processes, call them A and B. Any process in group A can send
              a message to or receive a message from any process in group B, and vice-versa. However, there
              is no way to use an intercommunicator to send messages among the processes within a group.
              Intercommunicators are often useful in large MPI applications that tie together many, smaller
              modules. Typically, each module will have its own intracommunicators and the modules will 
              interact with each other via intercommunicators.
            </summary>
        </member>
        <member name="M:MPI.Communicator.#ctor">
            <summary>
            Communicators can only be constructed from other communicators or adopted
            from low-level communicators.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI communicator that was created with any of the low-level MPI facilities.
              The resulting <c>Communicator</c> object will manage the lifetime of the low-level MPI communicator,
              and will free the communicator when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is creating MPI communicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Communicator.Finalize">
            <summary>
            Finalizer that frees the MPI communicator.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Dispose">
            <summary>
            Free the MPI communicator.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Create(MPI.Group)">
            <summary>
              Creates a new communicator containing all of the processes in the given group. 
              The resulting communicator may be <c>null</c>, if the calling process is not in this group.
            </summary>
            <param name="group">
              The group of processes that will be contained in the resulting communicator.
              This must be a subgroup of <see cref="P:MPI.Communicator.Group"/>.
            </param>
        </member>
        <member name="M:MPI.Communicator.Clone">
            <summary>
              Clones the communicator, creating a new (but distinct) communicator with the
              same processes.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Split(System.Int32,System.Int32)">
            <summary>
              Splits a communicator into several different communicators, each of which is identified
              by a particular color value. 
            </summary>
            <remarks>
              This is a collective operation, where each process may provide a different 
              <paramref name="color"/> and <paramref name="key"/>. Each distinct color value will 
              correspond to a new communicator, which contains all processes that provided that 
              specific color. The ranks of the processes in the resulting communicators will be 
              determined by the key value, with the lower ranks in the communicator assigned to
              the processes providing the smaller key values; ties are broken based on the ranks of
              the processes in the original communicator.
            </remarks>
            <param name="color">
               A non-negative color value, designating which new communicator the calling process
               will belong two. All processes that provide the same color will be a part of the 
               same resulting communicator.
            </param>
            <param name="key">
              A value that determines how processes with the same color are ranked in the resulting
              communicator. Smaller keys correspond to lower ranks, with ties broken by the ranks of
              the processes in the original communicator.
            </param>
            <returns>
              The communicator that the calling process is in, corresponding to its color value.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Abort(System.Int32)">
            <summary>
            Terminates all processes in this communicator. In most systems, this terminates all processes.
            </summary>
            <param name="errorcode">An error code that will be returned to the invoking environment.</param>
        </member>
        <member name="F:MPI.Communicator.world">
            <summary>
              The "world" communicator contains all of the processes that were
              originally created by MPI.
            </summary>
            <remarks>
              Use the "world" communicator to determine
              how many processes the user started for your MPI job, and as the 
              basis for creating the other communicators in your program. Many
              simple programs will use only the "world" communicator.
            </remarks>
        </member>
        <member name="F:MPI.Communicator.self">
            <summary>
              The "self" communicator is a simple communicator that contains only
              the currently-executing process. 
            </summary>
            <remarks>
              Each process will have a different "self" communicator, which only 
              permits communication with itself.
            </remarks>
        </member>
        <member name="M:MPI.Communicator.Compare(MPI.Communicator)">
            <summary>
              Compare two MPI communicators.
            </summary>
            <list>
              <listheader>
                <term>Value</term>
                <description>Description</description>
              </listheader>
            <item>
              <term><see cref="F:MPI.Comparison.Identical"/></term>
              <description>The two <c>Communicator</c> objects represent the same communicator.</description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Congruent"/></term>
              <description>
                The two <c>Communicator</c> objects contain the same processes with the same ranks,
                but represent different communicators.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Similar"/></term>
              <description>
                The two <c>Communicator</c> objects contain the same processes, but with different ranks.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Unequal"/></term>
              <descsription>The two <c>Communicator</c> objects are different.</descsription>
            </item>
            </list>
        </member>
        <member name="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)">
            <summary>
              Send a message to a particular processor.
            </summary>
            <remarks>
              The basic <c>Send</c> operation will block until this message data has been transferred from <c>value</c>. 
              This might mean that the <c>Send</c> operation will return immediately, before the receiver has actually received
              the data. However, it is also possible that <c>Send</c> won't return until it matches a <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/>
              operation. Thus, the <paramref name="dest"/> parameter should not be equal to <see cref="P:MPI.Communicator.Rank"/>, because a send-to-self
              operation might never complete. 
            </remarks>
            <typeparam name="T">The type of the value to be sent. This type must be serializable.</typeparam>
            <param name="value">The value that will be transmitted with the message.</param>
            <param name="dest">
              The rank of the process that will receive this message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
            </param>
            <param name="tag">A message "tag" that identifies this particular kind of message. The receive must use the same message tag to receive this message.</param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation, that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>
            The value received.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0@)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation, that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="value">
              When <c>Receive</c> completes, this parameter will contain the value
              transmitted as part of the message.
            </param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0@,MPI.CompletedStatus@)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation, that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="value">
              When <c>Receive</c> completes, this parameter will contain the value
              transmitted as part of the message.
            </param>
            <param name="status">
            Receives information about the completed receive operation.
            </param>
        </member>
        <member name="M:MPI.Communicator.Send``1(``0[],System.Int32,System.Int32)">
            <summary>
              Send a message to a particular processor.
            </summary>
            <remarks>
              The basic <c>Send</c> operation will block until this message data has been transferred from <c>values</c>. 
              This might mean that the <c>Send</c> operation will return immediately, before the receiver has actually received
              the data. However, it is also possible that <c>Send</c> won't return until it matches a <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@)"/>
              operation. Thus, the <paramref name="dest"/> parameter should not be equal to <see cref="P:MPI.Communicator.Rank"/>, because a send-to-self
              operation might never complete. 
            </remarks>
            <typeparam name="T">The type of the value to be sent. This type must be serializable.</typeparam>
            <param name="values">The values that will be transmitted with the message.</param>
            <param name="dest">
              The rank of the process that will receive this message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
            </param>
            <param name="tag">A message "tag" that identifies this particular kind of message. The receive must use the same message tag to receive this message.</param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@)">
            <summary>
              Receive a message from another process.
            </summary>
            <remarks>
              The <c>Receive</c> operation is a blocking operation, that will not complete until it has
              completely received a message.
            </remarks>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="values">
              When <c>Receive</c> completes, this parameter will contain the values
              transmitted as part of the message. The <paramref name="values"/> array
              must be large enough to contain all of the values that will be received.
            </param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@,MPI.CompletedStatus@)">
            <summary>
              Receive an array from another process.
            </summary>
            <remarks>
              The <c>Receive</c> operation is a blocking operation, that will not complete until it has
              completely received a message.
            </remarks>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="values">
              When <c>Receive</c> completes, this parameter will contain the values
              transmitted as part of the message. The <paramref name="values"/> array
              must be large enough to contain all of the values that will be received.
            </param>
            <param name="status">Contains information abot the received message.</param>
        </member>
        <member name="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)">
            <summary>
            Non-blocking send of a single value. This routine will initiate communication and
            then return immediately with a <see cref="T:MPI.Request"/> object that can be used to
            query the status of the communication. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value that will be transmitted.</param>
            <param name="dest">The rank of the destination process.</param>
            <param name="tag">The tag used to identify this message.</param>
            <returns>
              A new request object that can be used to manipulate this non-blocking
              communication.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateSend``1(``0[],System.Int32,System.Int32)">
            <summary>
            Non-blocking send of an array of data. This routine will initiate communication and
            then return immediately with a <see cref="T:MPI.Request"/> object that can be used to
            query the status of the communication. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values that will be transmitted. If <c>T</c> is a value type, 
              do not change the values in this array before the resulting request has been 
              completed, because the implementation may send these values at any time.
            </param>
            <param name="dest">The rank of the destination process.</param>
            <param name="tag">The tag used to identify this message.</param>
            <returns>
              A new request object that can be used to manipulate this non-blocking
              communication.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32)">
            <summary>
            Non-blocking receive of a single value. This routine will initiate a request to receive
            data and then return immediately. The data may be received in the background. To test for
            or force the completion of the communication, then access the received data, use the 
            returned <see cref="T:MPI.ReceiveRequest"/> object.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="source">
              Rank of the source process to receive data from. Alternatively, use <see cref="F:MPI.Communicator.anySource"/> to
              receive a message from any other process.
            </param>
            <param name="tag">
              The tag that identifies the message to be received. Alternatively, use <see cref="F:MPI.Communicator.anyTag"/>
              to receive a message with any tag.
            </param>
            <returns>
              A request object that allows one to test or force the completion of this receive request,
              and retrieve the resulting value.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32,``0[])">
            <summary>
            Non-blocking receive of an array of values. This routine will initiate a request to receive
            data and then return immediately. The data may be received in the background. To test for
            or force the completion of the communication, then access the received data, use the 
            returned <see cref="T:MPI.ReceiveRequest"/> object.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="source">
              Rank of the source process to receive data from. Alternatively, use <see cref="F:MPI.Communicator.anySource"/> to
              receive a message from any other process.
            </param>
            <param name="tag">
              The tag that identifies the message to be received. Alternatively, use <see cref="F:MPI.Communicator.anyTag"/>
              to receive a message with any tag.
            </param>
            <param name="values">
              An array into which the values will be received. This array must be large enough to
              accommodate all of the data sent by the source process.
            </param>
            <returns>
              A request object that allows one to test or force the completion of this receive request,
              and retrieve the resulting value. The object retrieved from the request object will be
              <paramref name="values"/>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Probe(System.Int32,System.Int32)">
            <summary>
            Wait for a message from the given source and with the specified tag to become
            available, but don't try to receive the message. This routine will wait indefinitely
            for a message meeting the given criteria to arrive, so it should only be invoked
            when you know a message is coming. If you just want to check whether a message is
            available use <see cref="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)"/>.
            </summary>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), or the special value <see cref="F:MPI.Communicator.anySource"/>. If it is
              <see cref="F:MPI.Communicator.anySource"/>, then we can match a message sent by any other process in
              this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be matched by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>A <see cref="T:MPI.Status"/> object containing information about the message.</returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)">
            <summary>
            Determine whether a message from the given source and with the specified tag is
            available, but don't try to receive the message. This routine will return 
            immediately, regardless of whether a message is available, so it is useful for
            polling to determine whether any messages need to be handled at this time. If
            your program can't do any work until a message arrives (and the message is 
            guaranteed to arrive, eventually), use <see cref="M:MPI.Communicator.Probe(System.Int32,System.Int32)"/> instead.
            </summary>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), or the special value <see cref="F:MPI.Communicator.anySource"/>. If it is
              <see cref="F:MPI.Communicator.anySource"/>, then we can match a message sent by any other process in
              this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be matched by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>
              If a message is available, a <see cref="T:MPI.Status"/> object containing information about that message.
              Otherwise, returns <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Allgather``1(``0)">
            <summary>
            Gathers invididual values provided by each processor into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available to all of the processors. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value supplied by this processor, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the returned array.
            </param>
            <returns>
              An array of values, where the ith value in the array is a copy of the <paramref name="value"/>
              argument provided by the processor with rank i.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Allgather``1(``0,``0[]@)">
            <summary>
            Gathers invididual values provided by each processor into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">
              The value supplied by this processr, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the returned array.
            </param>
            <param name="outValues">
              An array of values, where the ith value in the array is a copy of the <paramref name="inValue"/>
              argument provided by the processor with rank i.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})">
             <summary>
               <c>Allreduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
               single value available to all processes. The values are combined in a user-defined way, specified via 
               a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> are the values provided by the 
               N processes in the communicator, the result will be the value <c>value1 op value2 op ... op valueN</c>.
             
               An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> 
               followed by a <see cref="M:MPI.Communicator.Broadcast``1(``0@,System.Int32)"/>.
             </summary>
             <example>
               This example computes the sum of the ranks of all of the processes using 
               <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
               <code>
             using System;
             using MPI;
             
             class Allreduce
             {
               static void Main(string[] args)
               {
                 using (MPI.Environment env = new MPI.Environment(ref args))
                 {
                   Communicator world = Communicator.world;
            
                   int sum = world.Allreduce(world.Rank, Operation&lt;int&gt;.Add);
                   System.Console.WriteLine("Sum of ranks = " + sum);
                 }
               }
             };
               </code>
             </example>
             <typeparam name="T">Any serializable type.</typeparam>
             <param name="value">The local value that will be combined with the values provided by other processes.</param>
             <param name="op">
               The operation used to combine two values. This operation must be associative.
             </param>
             <returns>The result of the reduction. The same value will be returned to all processes.</returns>
        </member>
        <member name="M:MPI.Communicator.Allreduce``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>Allreduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
              single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. When provided with arrays, <c>Allreduce</c> combines the ith value of each of the arrays 
              passed to each process. Thus, <c>Allreduce</c> on arrays is the same as calling 
              <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/> for each <c>inValues[i]</c>.
            
              An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              followed by a <see cref="M:MPI.Communicator.Broadcast``1(``0[]@,System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <returns>
              The values that result from combining all of the values in <paramref name="values"/>
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>Allreduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
              single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. When provided with arrays, <c>Allreduce</c> combines the ith value of each of the arrays 
              passed to each process. Thus, <c>Allreduce</c> on arrays is the same as calling 
              <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/> for each <c>inValues[i]</c>.
            
              An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              followed by a <see cref="M:MPI.Communicator.Broadcast``1(``0[]@,System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="outValues">
              The values that result from combining all of the values in <paramref name="inValues"/>
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Communicator.Alltoall``1(``0[])">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Communicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values that will be sent to each process. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <returns>
              An array of values received from all of the other processes. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Alltoall``1(``0[],``0[]@)">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Communicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <param name="outValues">
              The array of values received from all of the other processes. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Communicator.Barrier">
            <summary>
              Wait until all processes in the communicator have reached the same barrier.
            </summary>
            <remarks>
              This collective operation should be called by all processes within the communicator.
              Each process will wait within the barrier until all processes have entered the barrier,
              at which point all of the processes will be released. Use barriers when all of the 
              processes need to synchronize at some particular point in your program flow.
            </remarks>
            <example>
              Barriers are often used after each process has completed some large, independent
              chunk of work:
            <code>
              public void Superstep(Communicator comm) 
              {
                // Perform a large chunk of work  locally
                DoLocalWork();
            
                // Synchronize with everyone else
                comm.Barrier();
            
                // Okay, move on to the next piece of work.
              }
            </code>
            </example>
        </member>
        <member name="M:MPI.Communicator.Broadcast``1(``0@,System.Int32)">
            <summary>
            Broadcast a value from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the value out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Communicator.Broadcast``1(``0[]@,System.Int32)">
            <summary>
            Broadcast an array from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root. Note that the receiving processes must already have allocated
              enough space in the array to receive data from the root, prior to calling <c>Broadcast</c>.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the values out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Communicator.Gather``1(``0,System.Int32)">
            <summary>
              Gather the values from each process into an array of values at the 
              <paramref name="root"/> process. On the root process, the pth element of the result
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> when this routine returns.
            </summary>
            <example>
            This example demonstrates the use of <c>Gather</c> to gather the ranks of 
            all processes together at root 0.
            <code>
            using System;
            using MPI;
            
            class Gather
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        Communicator world = Communicator.world;
                        if (world.Rank == 0)
                        {
                            int[] ranks = world.Gather(world.Rank, 0);
                            System.Console.Write("Ranks of all of the processes:");
                            for (int i = 0; i &lt; ranks.Length; ++i)
                                System.Console.Write(" " + i);
                            System.Console.WriteLine();
                        }
                        else
                        {
                            world.Gather(world.Rank, 0);
                        }
                    }
                }
            }
            </code>
            </example>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by this process.</param>
            <param name="root">
              The rank of the process that will receive values for all of the
              processes in the communicator.
            </param>
            <returns>
              At the root, an array containing the <paramref name="value"/>s supplied by each of the processes
              in the communicator. All other processes receive <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Gather``1(``0,System.Int32,``0[]@)">
            <summary>
              Gather the values from each process into an array of values at the 
              <paramref name="root"/> process. On the root process, <c>outValues[p]</c>
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> when this routine returns. Use this variant of the routine 
              when you want to pre-allocate storage for the <paramref name="outValues"/> array.
            </summary>
            <example>
            This example demonstrates the use of <c>Gather</c> to gather the ranks of 
            all processes together at root 0.
            <code>
            using System;
            using MPI;
            
            class Gather
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        Communicator world = Communicator.world;
                        if (world.Rank == 0)
                        {
                            int[] ranks = new int[world.Size];
                            world.Gather(world.Rank, 0, ref ranks);
                            System.Console.Write("Ranks of all of the processes:");
                            for (int i = 0; i &lt; ranks.Length; ++i)
                                System.Console.Write(" " + i);
                            System.Console.WriteLine();
                        }
                        else
                        {
                            world.Gather(world.Rank, 0);
                        }
                    }
                }
            }
            </code>
            </example>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value contributed by this process.</param>
            <param name="root">
              The rank of the process that will receive values for all of the
              processes in the communicator.
            </param>
            <param name="outValues">
              An array that will store the values contributed by each process.
              This value is only significant at the <paramref name="root"/>, and
              can be omitted by non-root processes.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)">
             <summary>
               <c>Reduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
               single value available at the designated <paramref name="root"/> process. The values are combined 
               in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
               are the values provided by the N processes in the communicator, the result will be the value 
               <c>value1 op value2 op ... op valueN</c>. This result is only
               available to the <paramref name="root"/> process. If all processes need the result of the reduction,
               use <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
             </summary>
             <example>
               This example computes the sum of the ranks of all of the processes using 
               <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/>.
               <code>
             using System;
             using MPI;
             
             class Reduce
             {
               static void Main(string[] args)
               {
                 using (MPI.Environment env = new MPI.Environment(ref args))
                 {
                   Communicator world = Communicator.world;
            
                   int root = 0;
                   if (world.Rank == root)
                   {
                     int sum;
                     world.Reduce(world.Rank, out sum, Operation&lt;int&gt;.Add, root);
                     System.Console.WriteLine("Sum of ranks = " + sum);
                   }
                   else 
                   {
                     world.Reduce(world.Rank, Operation&lt;int&gt;.Add, root);
                   }
                 }
               }
             };
               </code>
             </example>
             <typeparam name="T">Any serializable type.</typeparam>
             <param name="value">The local value that will be combined with the values provided by other processes.</param>
             <param name="op">
               The operation used to combine two values. This operation must be associative.
             </param>
             <param name="root">
               The rank of the process that is the root of the reduction operation, which will receive the result
               of the reduction operation in its <paramref name="outValue"/> argument.
             </param>
             <returns>
               On the root, returns the result of the reduction operation. The other processes receive a default value.
             </returns>
        </member>
        <member name="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)">
            <summary>
              <c>Reduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Communicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              The rank of the process that is the root of the reduction operation, to which this routine will
              return the result of the reduction operation. 
            </param>
            <returns>
              On the root, an array that contains the result of elementwise reduction on the <paramref name="values"/>
              arrays provided by each process. On all other processes, <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)">
            <summary>
              <c>Reduce</c>c&gt; is a collective algorithm that combines the values stored by each process into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Communicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              The rank of the process that is the root of the reduction operation, which will receive the result
              of the reduction operation in its <paramref name="outValue"/> argument.
            </param>
            <param name="outValues">
              The variable that will receive the results of the reduction operation. Only the <paramref name="root"/>
              process will receive the actual result; all other processes will receive a default-initialized value.
              When this collective operation returns, <paramref name="outValues"/> will be of the same length as
              <paramref name="inValues"/>.
            </param>
        </member>
        <member name="M:MPI.Communicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[])">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/> and then scatters
            the results by sending some elements to each process. The reduction will be performed on 
            the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values that will be reduced. The length of this array must equal to 
              sum of the counts in <paramref name="counts"/>.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="values"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. The 
            </param>
            <returns>
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[],``0[]@)">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> and then scatters
            the results by sending some elements to each process. The reduction will be performed on 
            the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              An array of values that will be reduced. The length of this array must equal to 
              sum of the counts in <paramref name="counts"/>.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="inValues"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. The 
            </param>
            <param name="outValues">
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </param>
        </member>
        <member name="M:MPI.Communicator.Scatter``1(``0[])">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            This variant of <c>Scatter</c> can only be called by the root process. Other processes
            should either call the non-root variant of <see cref="M:MPI.Communicator.Scatter``1(System.Int32)"/> or the
            general <see cref="M:MPI.Communicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values of length <see cref="P:MPI.Communicator.Size"/>, which is only significant at the root. 
              The ith value of this array (at the root process) will be sent to the ith processor.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Scatter``1(System.Int32)">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            This variant of <c>Scatter</c> can only be called by a non-root process. The root process
            should either call the root-only variant of <see cref="M:MPI.Communicator.Scatter``1(``0[])"/> or the
            general <see cref="M:MPI.Communicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Scatter``1(``0[],System.Int32)">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values of length <see cref="P:MPI.Communicator.Size"/>, which is only significant at the root. 
              The ith value of this array (at the root process) will be sent to the ith processor.
            </param>
            <param name="root">
              Rank of the "root" process, which will supply the array of values to be scattered.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="F:MPI.Communicator.comm">
            <summary>
            The low-level MPI communicator handle.
            </summary>
        </member>
        <member name="F:MPI.Communicator.shadowComm">
            <summary>
            Secondary low-level MPI communicator that will be used for communication of
            data within MPI.NET, such as collectives implemented over point-to-point or
            the data message associated with a Send of serialized data.
            </summary>
        </member>
        <member name="F:MPI.Communicator.tagAllocator">
            <summary>
            Tag allocator used to allocate tags for collective operations and serialized
            send operations.
            </summary>
        </member>
        <member name="F:MPI.Communicator.anySource">
            <summary>
              Special value for the <c>source</c> argument to <c>Receive</c>c> that
              indicates that the message can be received from any process in the communicator.
            </summary>
        </member>
        <member name="F:MPI.Communicator.anyTag">
            <summary>
              Special value for the <c>tag</c> argument to <c>Receive</c> that
              indices that the message with any tag can be received by the receive operation.
            </summary>
        </member>
        <member name="P:MPI.Communicator.Size">
            <summary>
              Returns the number of processes within this communicator. 
            </summary>
            <remarks>
              All of the processes within this communicator will have the same 
              value for Size.
            </remarks>
        </member>
        <member name="P:MPI.Communicator.Rank">
            <summary>
              Returns the rank of the currently executing process within this
              communicator. 
            </summary>
            <remarks>
              The rank is a number between 0 and Size-1 that is used 
              to identify this process, e.g., to send messages to it. The value of
              Rank will differ from one process to the next.
            </remarks>
        </member>
        <member name="P:MPI.Communicator.Group">
            <summary>
            Retrieve the group containing all of the processes in this communicator.
            </summary>
        </member>
        <member name="M:MPI.Intercommunicator.#ctor">
            <summary>
            Intercommunicators can only be constructed from other communicators or adopted
            from low-level intercommunicators.
            </summary>
        </member>
        <member name="M:MPI.Intercommunicator.#ctor(MPI.Intracommunicator,System.Int32,MPI.Intracommunicator,System.Int32,System.Int32)">
            <summary>
            Constructs a new intercommunicator that provides communication between two groups of processes,
            specified by intracommunicators. This is a collective operation involving all of the processes
            in the communicators passed via <paramref name="localComm"/>.
            </summary>
            <param name="localComm">
              Intracommunicator whose processes will form the local group in the resulting 
              intercommunication. Since there are two groups in an intercommunicator (each of which is
              identified by an intracommunicator), each process will provide the communicator for its
              own "local" group. Note that a process cannot be in both groups.
            </param>
            <param name="localLeader">
              The rank of the "leader" in the local group. Each of the processes must provide
              the same leader rank as all of the other processes within its group. 
            </param>
            <param name="bridgeComm">
              A communicator that permits communication between the leaders of the two groups. 
              This communicator is used internally to construct the intercommunicator, so there must
              not be any pending communication that might interfere with point-to-pointer messages with
              tag <paramref name="tag"/>. This parameter is only significant for the processes
              that are leaders of their group.
            </param>
            <param name="remoteLeader">
              The rank of the remote group's leader within the communicator <paramref name="bridgeComm"/>.
            </param>
            <param name="tag">
              A tag value that indicates which intercommunicator is used to transmit setup messages across
              the communicator <paramref name="bridgeComm"/>. Pick a tag value that will not interfere
              with other communicator on <paramref name="bridgeComm"/>.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI intercommunicator that was created with any of the low-level MPI facilities.
              The resulting <c>Intercommunicator</c> object will manage the lifetime of the low-level MPI intercommunicator,
              and will call <see cref="M:MPI.Unsafe.MPI_Comm_free(System.Int32*)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI intercommunicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Intercommunicator.Merge(System.Boolean)">
            <summary>
            Merge all of the processes in both groups of an intercommunicator into a single
            intracommunicator. While the intercommunicator only allows communication among processes
            in different groups, the merged intracommunicator allows communication among all of the 
            processes in either group, regardless of which group the source and target were in.
            </summary>
            <param name="upper">
              All of the processes within each group should provide the same value. If one group's
              processes provide the value <c>true</c> while the other group's processes provide the value
              <c>false</c>, the group providing the value <c>true</c> will receive the higher-numbered
              ranks in the resulting intracommunicator. If all processes provide the same value, 
              the order of the union is arbitrary.
            </param>
            <returns>The new intracommunicator.</returns>
        </member>
        <member name="P:MPI.Intercommunicator.RemoteSize">
            <summary>
              Returns the number of processes in the remote group of this intercommunicator. 
            </summary>
        </member>
        <member name="P:MPI.Intercommunicator.RemoteGroup">
            <summary>
            Retrieve the group containing all of the remote processes in this intercommunicator.
            </summary>
        </member>
        <member name="T:MPI.Status">
            <summary>
            Contains information about a specific message transmitted via MPI.
            </summary>
        </member>
        <member name="M:MPI.Status.#ctor(MPI.Unsafe.MPI_Status)">
            <summary>
              Constructs a <code>Status</code> object from a low-level <see cref="T:MPI.Unsafe.MPI_Status"/> structure.
            </summary>
        </member>
        <member name="M:MPI.Status.Count(System.Type)">
            <summary>
            Determine the number of elements transmitted by the communication
            operation associated with this object.
            </summary>
            <param name="type">
              The type of data that will be stored in the message.
            </param>
            <returns>
              If the type of the data is a value type, returns the number
              of elements in the message. Otherwise, returns <c>null</c>,
              because the number of elements stored in the message won't
              be known until the message is received.
            </returns>
        </member>
        <member name="F:MPI.Status.status">
            <summary>
             The low-level MPI status object.
            </summary>
        </member>
        <member name="P:MPI.Status.Source">
            <summary>
            The rank of the process that sent the message.
            </summary>
        </member>
        <member name="P:MPI.Status.Tag">
            <summary>
            The tag used to send the message.
            </summary>
        </member>
        <member name="P:MPI.Status.Cancelled">
            <summary>
            Whether the communication was cancelled before it completed.
            </summary>
        </member>
        <member name="T:MPI.CompletedStatus">
            <summary>
            Information about a specific message that has already been
            transferred via MPI.
            </summary>
        </member>
        <member name="M:MPI.CompletedStatus.#ctor(MPI.Unsafe.MPI_Status,System.Int32)">
            <summary>
              Constructs a <code>Status</code> object from a low-level <see cref="T:MPI.Unsafe.MPI_Status"/> structure
              and a count of the number of elements received.
            </summary>
        </member>
        <member name="M:MPI.CompletedStatus.Count">
            <summary>
            Determines the number of elements in the transmitted message.
            </summary>
        </member>
        <member name="F:MPI.CompletedStatus.count">
            <summary>
            The number of elements in the message.
            </summary>
        </member>
        <member name="T:MPI.Operation`1">
            <summary>
            The <c>Operation</c> class provides reduction operations for use with the
            reduction collectives in the <see cref="T:MPI.Communicator"/> class, such as 
            <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>. For example, 
            the <see cref="P:MPI.Operation`1.Add"/> property is a delegate that adds two values of type
            <typeparamref name="T"/>, while <see cref="P:MPI.Operation`1.Min"/> returns the minimum of the
            two values. The reduction operations provided by this class should be 
            preferred to hand-written reduction operations (particularly for built-in types)
            because it enables additional optimizations in the MPI library.
            
            The <c>Operation</c> class also has a second role for users that require
            access to the low-level MPI interface. Creating an instance of the <c>Operation</c>
            class will find or create an appropriate <c>MPI_Op</c> for that reduction
            operation. This <c>MPI_Op</c>, accessible through the <see cref="P:MPI.Operation`1.Op"/> property,
            can be used with low-level MPI reduction operations directly.
            </summary>
            <typeparam name="T">The type of data used in these operations.</typeparam>
        </member>
        <member name="M:MPI.Operation`1.BuildMethod(System.String,System.Reflection.Emit.OpCode,System.String)">
            <summary>
            Synthesize a new mpiDelegateMethod associated with a given operation.
            </summary>
            <param name="name">The name of the mpiDelegateMethod to build, e.g., "add".</param>
            <param name="opcode">The <c>OpCode</c> used for primitive types.</param>
            <param name="methodName">The name of the overloaded mpiDelegateMethod used for class types.</param>
            <returns>Returns a new delegate implementing the given operation.</returns>
        </member>
        <member name="F:MPI.Operation`1.min">
            <summary>
            Cached intermediate of the reduction operation that computes the minimum of 
            two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.max">
            <summary>
            Cached intermediate of the reduction operation that computes the maximum of 
            two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.add">
            <summary>
            Cached intermediate of the reduction operation that adds two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.multiply">
            <summary>
            Cached intermediate of the reduction operation that multiplies two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.logicalAnd">
            <summary>
            Cached intermediate of the reduction operation that computes the logical
            AND of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.logicalOr">
            <summary>
            Cached intermediate of the reduction operation that computes the logical
            OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.bitwiseAnd">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            AND of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.bitwiseOr">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.exclusiveOr">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            exclusive OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.Kind">
            <summary>
            The MPI datatype classification of the type <typeparamref name="T"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.GetDatatypeKind">
            <summary>
              Retrieves the kind of datatype for the type <typeparamref name="T"/>. 
              Used only to initialize <see cref="F:MPI.Operation`1.Kind"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.GetPredefinedOperation(MPI.ReductionOperation{`0})">
            <summary>
            Determine the predefined <c>MPI_Op</c> that is associated with
            this reduction operation. If no such <c>MPI_Op</c> exists, 
            returns <see cref="F:MPI.Unsafe.MPI_OP_NULL"/>.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.UseGeneratedUserOps">
            <summary>
            When true, we will use user-defined MPI operations generated on-the-fly for
            reduction operations on value types. Otherwise, we will use the more generic,
            static MPI operations.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.mpiDelegateMethod">
            <summary>
            The dynamically-generated method that provides a delegate with a
            signature compatible with <c>MPI_User_function</c> that calls
            the user's <see cref="T:MPI.ReductionOperation`1"/> repeatedly.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.MakeMPIDelegate(MPI.ReductionOperation{`0})">
            <summary>
            Creates a new MPI delegate from a reduction operation.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.#ctor(MPI.ReductionOperation{`0})">
            <summary>
            Create a user-defined MPI operation based on the given reduction operation.
            </summary>
            <param name="op">The reduction operation.</param>
        </member>
        <member name="M:MPI.Operation`1.Dispose">
            <summary>
            Free the MPI operation that this object wraps, but only if it is not a 
            predefined MPI operation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.wrapper">
            <summary>
            The wrapper around the user's reduction operation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.mpiOp">
            <summary>
            The actual <c>MPI_Op</c> corresponding to this operation.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Min">
            <summary>
            Reduction operation that computes the minimum of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Max">
            <summary>
            Reduction operation that computes the maximum of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Add">
            <summary>
            Reduction operation that adds two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Multiply">
            <summary>
            Reduction operation that multiply two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.LogicalAnd">
            <summary>
            Reduction operation that computes the logical AND of two values,
            including integral types.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.LogicalOr">
            <summary>
            Reduction operation that computes the logical OR of two values,
            including integral types.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.BitwiseAnd">
            <summary>
            Reduction operation that computes the bitwise AND of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.BitwiseOr">
            <summary>
            Reduction operation that computes the bitwise OR of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.ExclusiveOr">
            <summary>
            Reduction operation that computes the bitwise exclusive OR of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Op">
            <summary>
            The MPI operation that can corresponds to the user's reduction operation,
            This operation may be either a predefined MPI reduction operation or a 
            user-defined <c>MPI_Op</c> created by the <c>Operation</c> constructor.
            </summary>
        </member>
        <member name="T:MPI.Operation`1.DatatypeKind">
            <summary>
            The kind of MPI datatype. MPI classifies the predefined data types into 
            several different categories. This classification is used primarily to determine 
            which predefined reduction operations are supported by the low-level MPI
            implementation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.CInteger">
            <summary>
            C integer types, such as the predefined integral types in C# and .NET.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.FortranInteger">
            <summary>
            Fortran integer types. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.FloatingPoint">
            <summary>
            Floating point types, such as the predefined <c>float</c> and <c>double</c> types.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Logical">
            <summary>
            The MPI logical type. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Complex">
            <summary>
            The MPI complex type. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Byte">
            <summary>
            The MPI byte type, which corresponds to the <c>byte</c> type in C#.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Other">
            <summary>
            Any type that does not fit into one of the other MPI datatype classifications.
            </summary>
        </member>
        <member name="T:MPI.Operation`1.WrapReductionOperation">
            <summary>
            Wraps a <see cref="T:MPI.ReductionOperation`1"/> to provide an MPI-compatible interface
            for use with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32*)"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.WrapReductionOperation.#ctor(MPI.ReductionOperation{`0})">
            <summary>
            Construct a new reduction operation wrapper.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.WrapReductionOperation.Apply(System.Void*,System.Void*,System.Int32*,System.Int32*)">
            <summary>
            Applies a reduction operation to each of the values in <paramref name="invec"/> and 
            <paramref name="inoutvec"/>, writing the results back into the corresponding
            position in <paramref name="inoutvec"/>.
            </summary>
            <param name="invec">Incoming values of type <c>T</c>.</param>
            <param name="inoutvec">Incoming values of type <c>T</c>. The results of the reduction
            operation will be written back to this memory.</param>
            <param name="len">
              The length of the <paramref name="invec"/> and <paramref name="inoutvec"/> arrays.
            </param>
            <param name="datatype">
              The MPI datatype for the data stored in <paramref name="invec"/> and <paramref name="outvec"/>.
              This should be the same as the intermediate of <see cref="M:MPI.DatatypeCache.GetDatatype(System.Type)"/> applied to the
              type <c>T</c>.
            </param>
        </member>
        <member name="T:MPI.ReductionOperation`1">
            <summary>
              A reduction operation that combines two values to produce a third value.
              Used by various collectives operations such as 
              <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
            </summary>
        </member>
        <member name="T:MPI.SerializedMessageHeader">
            <summary>
            Structure sent on the main MPI communicator for the <see cref="T:MPI.Communicator"/>
            class to indicate that serialized data will be coming on the shadow communicator.
            </summary>
        </member>
        <member name="F:MPI.SerializedMessageHeader.tag">
            <summary>
            The tag of the message containing the serialized data, which will come via
            the shadow communicator.
            </summary>
        </member>
        <member name="F:MPI.SerializedMessageHeader.bytes">
            <summary>
            Number of bytes in the serialized data.
            </summary>
        </member>
        <member name="T:MPI.Comparison">
            <summary>
              The intermediate of a comparison between two MPI objects.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Identical">
            <summary>
            The two MPI objects are identical.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Congruent">
            <summary>
            The two MPI objects are not identical, but they have the same properties.
            For example, two <see cref="T:MPI.Communicator"/>s that contain the same set of
            processes with the same ranks.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Similar">
            <summary>
            The two MPI objects are similar, but are not identical and do not match
            exactly. For example, two <see cref="T:MPI.Communicator"/>s that contain the 
            same set of processes but with different rank order.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Unequal">
            <summary>
            The two MPI objects are different.
            </summary>
        </member>
        <member name="T:MPI.Unsafe">
            <summary>
            Direct, low-level interface to the system MPI library.
            
            This low-level interface provides direct access to the unmanaged
            MPI library provided by the system. It is by nature unsafe, and
            should only be used by programmers experienced both in the use 
            of MPI from lower-level languages (e.g., C, Fortran) and with an
            understanding of the interaction between managed and unmanaged
            code, especially those issues that pertain to memory 
            pinning/unpinning.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_WORLD">
            <summary>
              Predefined communicator containing all of the MPI processes. See <see cref="F:MPI.Communicator.world"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_SELF">
            <summary>
              Predefined communicator containing only the calling process. See <see cref="F:MPI.Communicator.self"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_NULL">
            <summary>
              Predefined communicator representing "no communicator". In the higher-level interface, this
              is represented by a <c>null</c> <see cref="T:MPI.Communicator"/> object.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_CHAR">
            <summary>
            A single character. There is no equivalent to this type in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SIGNED_CHAR">
            <summary>
            A single, signed character. This is equivalent to the <c>sbyte</c> type
            in C# and the <c>System.SByte</c> type in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_CHAR">
            <summary>
            A single, unsigned character. There is no equivalent to this type in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BYTE">
            <summary>
            A single byte. This is equivalent to the <c>byte</c> type
            in C# and the <c>System.Byte</c> type in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_WCHAR">
            <summary>
            A single, wide character. The equivalent is <c>char</c> in C# and <c>System.Char</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SHORT">
            <summary>
            A signed short integer. This is equivalent to the <c>short</c> type in C# and
            <c>System.Int16</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_SHORT">
            <summary>
            An unsigned short integer. This is equivalent to the <c>ushort</c> type in C# and
            <c>System.UInt16</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_INT">
            <summary>
            A signed integer. This is equivalent to the <c>int</c> type in C# and
            <c>System.Int32</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED">
            <summary>
            An unsigned integer. This is equivalent to the <c>uint</c> type in C# and
            <c>System.UInt32</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG">
            <summary>
            A long signed integer. There is no equivalent in C# or .NET, because the 64-bit 
            integer in C# and .NET is mapped to <see cref="F:MPI.Unsafe.MPI_LONG_LONG_INT"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_LONG">
            <summary>
            A long unsigned integer. There is no equivalent in C# or .NET, because the 64-bit 
            unsigned integer in C# and .NET is mapped to <see cref="F:MPI.Unsafe.MPI_UNSIGNED_LONG_LONG"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_FLOAT">
            <summary>
            A single-precision floating-point value. The equivalent is <c>float</c> in C#
            and <c>System.Single</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_DOUBLE">
            <summary>
            A double-precision floating-point value. The equivalent is <c>double</c> in C#
            and <c>System.Double</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_DOUBLE">
            <summary>
            An extended-precision floating-point value. There is no equivalent in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_LONG_INT">
            <summary>
            A long long signed integer. The equivalent is <c>long</c> in C# and
            <c>System.Int64</c> in .NET. This is a synonym for <see cref="F:MPI.Unsafe.MPI_LONG_LONG"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_LONG_LONG">
            <summary>
            A long long unsigned integer. The equivalent is <c>ulong</c> in C# and
            <c>System.UInt64</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_LONG">
            <summary>
            A long long signed integer. The equivalent is <c>long</c> in C# and
            <c>System.Int64</c> in .NET. This is a synonym for <see cref="F:MPI.Unsafe.MPI_LONG_LONG_INT"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PACKED">
            <summary>
            A special data type used to indicate data that has been packed with <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)"/>.
            This type is only used by the lowest-level MPI operations. The .NET equivalent is the 
            <see cref="T:MPI.DatatypeCache.Packed"/> type.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LB">
            <summary>
            A special marker that indicates the lower bound of an MPI datatype, and is typically
            only used by very low-level MPI operations that build special, derived datatypes.
            The .NET equivalent is the <see cref="T:MPI.DatatypeCache.LowerBound"/> type.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UB">
            <summary>
            A special marker that indices the upper bound of an MPI datatype, and is typically
            only used by very low-level MPI operations that build special, derived datatypes.
            The .NET equivalent is the <see cref="T:MPI.DatatypeCache.UpperBound"/> type.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_DATATYPE_NULL">
            <summary>
            A special datatype value that indicates "no datatype".
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_IDENT">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are identical.
            See <see cref="F:MPI.Comparison.Identical"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_CONGRUENT">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are congruent,
            meaning that the objects act the same way but are not identical.
            See <see cref="F:MPI.Comparison.Congruent"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SIMILAR">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are similar,
            but assign different ranks to each of the processes.
            See <see cref="F:MPI.Comparison.Similar"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNEQUAL">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are completely
            different.
            See <see cref="F:MPI.Comparison.Unequal"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PROC_NULL">
            <summary>
            Special value for the source or dest argument to any communication operation,
            which indicates that the communication is a no-op. Not supported in MPI.NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_GROUP_EMPTY">
            <summary>
            An empty group containing no processes. See <see cref="F:MPI.Group.empty"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_GROUP_NULL">
            <summary>
            A constant used to indicate the "null" group of processes. Corresponds to a null <see cref="T:MPI.Group"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNDEFINED">
            <summary>
            "Undefined" value used to identify when a rank is not a part of a group.
            See <see cref="F:MPI.Group.NoProcess"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_SINGLE">
            <summary>
            Indicates that the MPI program is single-threaded. See <see cref="F:MPI.Threading.Single"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_FUNNELED">
            <summary>
            Indicates that the MPI program is multi-threaded, but all MPI operations will be called
            from the main thread. See <see cref="F:MPI.Threading.Funneled"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_SERIALIZED">
            <summary>
            Indicates that the MPI program is multi-threaded, but only one thread will call into MPI
            at any given time. See <see cref="F:MPI.Threading.Serialized"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_MULTIPLE">
            <summary>
            Indicates that the MPI program is multi-threaded, and any thread can call into MPI
            at any time. See <see cref="F:MPI.Threading.Multiple"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAX_PROCESSOR_NAME">
            <summary>
            The maximum length of the string returned by <see cref="M:MPI.Unsafe.MPI_Get_processor_name(System.Byte*,System.Int32*)"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ANY_SOURCE">
            <summary>
            Predefined value for the "source" parameter to MPI receive or probe operations, 
            which indicates that a message from any process may be matched.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ANY_TAG">
            <summary>
            Predefined value for the "tag" parameter to MPI receive or probe operations,
            which indicates that a message with any tag may be matched.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAX">
            <summary>
            Compute the maximum value via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Max"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MIN">
            <summary>
            Compute the minimum value via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Min"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SUM">
            <summary>
            Compute the sum via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Add"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PROD">
            <summary>
            Compute the product via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.Multiply"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LAND">
            <summary>
            Compute the logical AND via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.LogicalAnd"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BAND">
            <summary>
            Compute the bitwise AND via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.BitwiseAnd"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LOR">
            <summary>
            Compute the logical OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.LogicalOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BOR">
            <summary>
            Compute the bitwise OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.BitwiseOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LXOR">
            <summary>
            Compute the logical exclusive OR via an MPI reduction operation.
            There is no high-level operation corresponding to this predefined
            MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BXOR">
            <summary>
            Compute the bitwise exclusive OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.ExclusiveOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MINLOC">
            <summary>
            Compute the minimum value and location of that value via
            an MPI reduction operation. There is no high-level operation
            corresponding to this predefined MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAXLOC">
            <summary>
            Compute the maximum value and location of that value via
            an MPI reduction operation. There is no high-level operation
            corresponding to this predefined MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_OP_NULL">
            <summary>
            Placeholder operation that indicates "no operation".
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_REQUEST_NULL">
            <summary>
            Constant that indicates a "null" MPI request, meaning that there is no such request.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BOTTOM">
            <summary>
            A special marker used for the "buf" parameter to point-to-point operations
            and some collectives that indicates that the derived datatype contains absolute 
            (rather than relative) addresses. The use of <c>MPI_BOTTOM</c> is not recommended.
            This facility is unused in C# and .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_STATUS_IGNORE">
            <summary>
            Constant used to indicate that the <c>MPI_Status</c> argument of an MPI
            operation will be ignored.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_STATUSES_IGNORE">
            <summary>
            Constant used to indicate that the array of <c>MPI_Status</c> arguments
            to an MPI operation will be ignored.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_contiguous(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Creates a new datatype from a contiguous block of values of the same type.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_vector(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
            Creates a new datatype from a strided block of values of the same type.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_hvector(System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*)">
            <summary>
            Creates a new datatype from a strided block of values of the same type.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_indexed(System.Int32,System.Int32*,System.Int32*,System.Int32,System.Int32*)">
            <summary>
            Creates a new datatype from discontiguous blocks of values of the same type.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_hindexed(System.Int32,System.Int32*,System.IntPtr*,System.Int32,System.Int32*)">
            <summary>
            Creates a new datatype from discontiguous blocks of values of the same type.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_struct(System.Int32,System.Int32*,System.IntPtr*,System.Int32*,System.Int32*)">
            <summary>
            Creates a new datatype from a structure containing discontiguous blocks of different types.
            This is the most general type constructor, and is used by the <see cref="T:MPI.DatatypeCache"/> to
            create MPI datatypes from .NET value types.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)">
            <summary>
            Packs (serializes) data into a byte buffer. This serialized representation can be transmitted via MPI
            with the datatype <see cref="F:MPI.Unsafe.MPI_PACKED"/>and unpacked with <see cref="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32*,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>. Serialization 
            in MPI.NET is automatic, so this routine is very rarely used.
            </summary>
            <param name="inbuf">
              Pointer to the input buffer, containing <paramref name="incount"/> values with the MPI datatype
              <paramref name="datatype"/>.
            </param>
            <param name="incount">The number of values in <paramref name="inbuf"/>.</param>
            <param name="datatype">The MPI datatype of the values in <paramref name="inbuf"/>.</param>
            <param name="outbuf">
              A pointer to the buffer of bytes into which we will be packing the serialized representation
              of the data in <paramref name="inbuf"/>.
            </param>
            <param name="outsize">The size (in bytes) of <paramref name="outbuf"/>.</param>
            <param name="position">
              A pointer to the position (in bytes) into <paramref name="outbuf"/> where the packed data 
              will be written. This position will be updated to the next available position in the buffer
              once the serialized data has been written.
            </param>
            <param name="comm">The communicator over which the packed data will be sent.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32*,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Unpacks (deserializes) data from a byte buffer. The serialized representation will have been
            packed by <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)"/> and (possibly) transmitted via MPI using the datatype 
            <see cref="F:MPI.Unsafe.MPI_PACKED"/>. Serialization in MPI.NET is automatic, so this routine is very
            rarely used.
            </summary>
            <param name="inbuf">A pointer to the buffer of bytes that will be unpacked.</param>
            <param name="insize">The number of bytes in <paramref name="inbuf"/>.</param>
            <param name="position">
              A pointer to the position (in bytes) inside the buffer from which data will be unpacked.
              This position will be updated to reflect the position of the next value in the buffer
              after the data has been unpacked.
            </param>
            <param name="outbuf">
            A pointer to the buffer where the unpacked data will be written. This buffer contains
            values whose MPI datatype is <paramref name="datatype"/>.
            </param>
            <param name="outcount">Number of values that will be stored into <paramref name="outbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="outbuf"/>.</param>
            <param name="comm">
              The communicator for which the packing was performed (and which may have been used to transmit the
              serialized data).
            </param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Pack_size(System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
            Determine the maximum amount of space that packing <paramref name="incount"/> values with the
            MPI datatype <paramref name="datatype"/> will require. This routine is useful for allocating
            buffer space when packing data with <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)"/>.
            </summary>
            <param name="incount">
              The number of elements of type <paramref name="datatype"/> to be packed.
            </param>
            <param name="datatype">The type of data to be packed.</param>
            <param name="comm">The communicator over which the packed data would be transmitted.</param>
            <param name="size">
              A pointer to an integer. This integer will receive the maximum number of bytes required to
              pack the data. However, it is possible that when calling <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)"/>, fewer bytes
              will be required to pack the actual data.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Address(System.IntPtr,System.IntPtr*)">
            <summary>
            Converts a pointer into an address for use with MPI. In many cases, this operation is simply a
            cast from the pointer's value to an integer.
            </summary>
            <param name="location">A pointer to the memory whose address will be returned.</param>
            <param name="address">
              A pointer to the integer address value that will be replaced with the address pointed to
              by <paramref name="location"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_extent(System.Int32,System.IntPtr*)">
            <summary>
            Determines the extent of the datatype.
            </summary>
            <param name="datatype">The datatype to query.</param>
            <param name="extent">Receives the extent of <paramref name="datatype"/>.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_size(System.Int32,System.Int32*)">
            <summary>
            Computes the size of a datatype.
            </summary>
            <param name="datatype">The MPI datatype.</param>
            <param name="size">
              Pointer to an integer, which will be assigned the size of the data type (in bytes).
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_commit(System.Int32*)">
            <summary>
            Completes creation of an MPI datatype. This routine will be called
            automatically when the MPI datatype is being generated via reflection
            in <see cref="M:MPI.DatatypeCache.GetDatatype(System.Type)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_free(System.Int32*)">
            <summary>
            Frees an MPI datatype.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_size(System.Int32,System.Int32*)">
            <summary>
            Determine the number of processes in a group. See <see cref="P:MPI.Group.Size"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_rank(System.Int32,System.Int32*)">
            <summary>
            Determine the rank of the calling process in a group. See <see cref="P:MPI.Group.Rank"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_translate_ranks(System.Int32,System.Int32,System.Int32*,System.Int32,System.Int32*)">
            <summary>
              Translate the ranks of processes in one group into those processes' corresponding
              ranks in another group. See <see cref="M:MPI.Group.TranslateRanks(System.Int32[],MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_compare(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Compare two groups. See <see cref="M:MPI.Group.Compare(MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_group(System.Int32,System.Int32*)">
            <summary>
            Retrieve the group associated with a communicator. See <see cref="P:MPI.Communicator.Group"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_union(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Create a group from the union of two groups. See <see cref="M:MPI.Group.op_BitwiseOr(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_intersection(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Create a group from the intersection of two groups. See <see cref="M:MPI.Group.op_BitwiseAnd(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_difference(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Create a group from the difference of two groups. See <see cref="M:MPI.Group.op_Subtraction(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_incl(System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
            Create a subgroup containing the processes with specific ranks in an existing group. 
             See <see cref="M:MPI.Group.IncludeOnly(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_excl(System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
            Create a subgroup containing all processes in existing group except those specified. 
            See <see cref="M:MPI.Group.Exclude(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_range_incl(System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
            Create a subgroup of processes in a group, based on a set of (first, last, stride) rank triples. 
            Note: this precise functionality is not exposed directly in the normal MPI layer; however, the
            same semantics can be attained with <see cref="M:MPI.Group.IncludeOnly(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_range_excl(System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
            Create a subgroup of processes containing all of the processes in the source group except those described
            by one of th provided(first, last, stride) rank triples. 
            Note: this precise functionality is not exposed directly in the normal MPI layer; however, the
            same semantics can be attained with <see cref="M:MPI.Group.Exclude(System.Int32[])"/>.
            </summary>        
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_free(System.Int32*)">
            <summary>
              Frees a group. This routine will be invoked automatically by <see cref="M:MPI.Group.Dispose"/>
              or the finalizer for <see cref="T:MPI.Group"/>.
            </summary>
            <param name="group"></param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_size(System.Int32,System.Int32*)">
            <summary>
            Determines the number of processes in the communicator. See <see cref="P:MPI.Communicator.Size"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_rank(System.Int32,System.Int32*)">
            <summary>
            Determines the rank of the calling process in the communicator. See <see cref="P:MPI.Communicator.Rank"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_compare(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Compare two communicators. See <see cref="M:MPI.Communicator.Compare(MPI.Communicator)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_dup(System.Int32,System.Int32*)">
            <summary>
            Duplicates a communicator, creating a new communicator with the same processes and ranks
            as the original. See <see cref="M:MPI.Communicator.Clone"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_create(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Creates a new communicator from a subgroup of the processes in an existing communicator.
            See <see cref="M:MPI.Communicator.Create(MPI.Group)"/>.
            </summary>
            <param name="comm">
              The existing communicator, from which the new communicator will be created.
            </param>
            <param name="group">
              A group specifying zero or more processes in the communicator <paramref name="comm"/>.
            </param>
            <param name="newcomm">
              A pointer that points to the newly-created communicator, which will contain all of
              the processes in the group. The order of the processes in this new communicator will 
              be the same as the order of those processes in the original communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_split(System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
              Splits a communicator into several new communicators, based on the colors provided.
              See <see cref="M:MPI.Communicator.Split(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_free(System.Int32*)">
            <summary>
              Frees a communicator. This routine will be invoked automatically by <see cref="M:MPI.Communicator.Dispose"/>
              or the finalizer for <see cref="T:MPI.Communicator"/>.
            </summary>
            <param name="comm"></param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_test_inter(System.Int32,System.Int32*)">
            <summary>
            Determine whether a communicator is an intercommunicator. In MPI.NET, intercommunicators 
            will have type <see cref="T:MPI.Intercommunicator"/>.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="flag">
              Pointer to a flag, which will be set to a non-zero value if <paramref name="comm"/>
              is an intercommunicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_remote_size(System.Int32,System.Int32*)">
            <summary>
            Determines the number of processes in the remote group of an intercommunicator.
            See <see cref="P:MPI.Intercommunicator.RemoteSize"/>.
            </summary>
            <param name="comm">The intercommunicator.</param>
            <param name="size">
              Will receive the number of processes in the remote group of 
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_remote_group(System.Int32,System.Int32*)">
            <summary>
            Retrieves the remote group from an intercommunicator.
            See <see cref="P:MPI.Intercommunicator.RemoteGroup"/>.
            </summary>
            <param name="comm">The intercommunicator.</param>
            <param name="group">
              Will receive the group containing all of the processes in the remote group 
              of <paramref name="comm"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Intercomm_create(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
            Create a new intercommunicator from two disjoint intracommunicators.
            See <see cref="M:MPI.Intercommunicator.#ctor(MPI.Intracommunicator,System.Int32,MPI.Intracommunicator,System.Int32,System.Int32)"/>.
            </summary>
            <param name="local_comm">The local communicator.</param>
            <param name="local_leader">The leader of the local communicator.</param>
            <param name="bridge_comm">
              Communicator that bridges the intercommunicators, allowing the leaders to communicate.
            </param>
            <param name="remote_leader">
              The rank of the remote group's leader within <paramref name="bridge_comm"/>.
            </param>
            <param name="tag">Tag used for communication to create the intercommunicator.</param>
            <param name="newintercomm">Will receive the new intercommunicator.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Intercomm_merge(System.Int32,System.Int32,System.Int32*)">
            <summary>
            Merge the two groups in an intercommunicator into a single intracommunicator.
            See <see cref="M:MPI.Intercommunicator.Merge(System.Boolean)"/>
            </summary>
            <param name="intercomm">The intercommunicator.</param>
            <param name="high">
              Whether this group of processes has the higher ranks in the resuling intercommunicator.
            </param>
            <param name="newintracomm">The resulting intracommunicator.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Init(System.Int32*,System.Byte***)">
            <summary>
            Initializes MPI. This routine must be called before any other MPI routine.
            It will be invoked by the <see cref="T:MPI.Environment"/> constructor.
            </summary>
            <param name="argc">The number of arguments in <paramref name="argv"/>.</param>
            <param name="argv">
              A pointer to an array of C-style strings containing all of
              the command-line arguments.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Finalize">
            <summary>
            Finalizes (shuts down) MPI. This routine must be called before exiting the
            program. It will be invoked by <see cref="M:MPI.Environment.Dispose"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Initialized(System.Int32*)">
            <summary>
            Determine whether MPI has already been initialized. See <seealso cref="P:MPI.Environment.Initialized"/>.
            </summary>
            <param name="flag">Will be set to a non-zero value if the MPI environment has been initialized.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Finalized(System.Int32*)">
            <summary>
            Determine whether MPI has already been finalized. See <seealso cref="P:MPI.Environment.Finalized"/>.
            </summary>
            <param name="flag">Will be set to a non-zero value if the MPI environment has been finalized.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Abort(System.Int32,System.Int32)">
            <summary>
            Aborts the current MPI program.
            </summary>
            <param name="comm">The communicator that will be used to abort the program.</param>
            <param name="errcode">The error code to be returned from the MPI process.</param>
            <returns>This routine does not return.</returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Init_thread(System.Int32*,System.Byte***,System.Int32,System.Int32*)">
            <summary>
            Initializes the MPI library with thread support. This operation subsumes <see cref="M:MPI.Unsafe.MPI_Init(System.Int32*,System.Byte***)"/>.
            See <see cref="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)"/>.
            </summary>
            <param name="argc">Pointer to the number of arguments passed on the command line.</param>
            <param name="argv">Pointer to the command-line arguments (array of C-style strings).</param>
            <param name="required">
              The threading level required by the caller, which must be one of the <c>MPI_THREAD_*</c> 
              constants.
            </param>
            <param name="provided">
              Returns the actual threading level that the MPI implementation is providing, which will be
              one of the <c>MPI_THREAD_*</c> constants.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Is_thread_main(System.Int32*)">
            <summary>
            Determine whether the calling thread is the main MPI thread (that called <see cref="M:MPI.Unsafe.MPI_Init(System.Int32*,System.Byte***)"/>
            or <see cref="M:MPI.Unsafe.MPI_Init_thread(System.Int32*,System.Byte***,System.Int32,System.Int32*)"/>. See <see cref="P:MPI.Environment.IsMainThread"/>.
            </summary>
            <param name="flag">Returns whether the calling thread is the main thread.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Query_thread(System.Int32*)">
            <summary>
            Determine the level of threading support provided by the MPI library.
            </summary>
            <param name="provided">Returns one of the <c>MPI_THREAD_*</c> constants.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Get_processor_name(System.Byte*,System.Int32*)">
            <summary>
            Retrieve the name of the processor or compute node that is currently executing.
            See <see cref="P:MPI.Environment.ProcessorName"/>.
            </summary>
            <param name="name">
              Pointer to an array of bytes that will, on return, contain the name of
              the currenly executing processor. If the processor name requires more
              than *<paramref name="len"/> ASCII characters, only the first 
              *<paramref name="len"/> characters will be written. To be sure that you
              will always get the full processor name, <paramref name="name"/> should
              refer to at least <see cref="F:MPI.Unsafe.MPI_MAX_PROCESSOR_NAME"/> characters.
            </param>
            <param name="len">
              A pointer to the number of bytes in the processor name. On input, this is 
              the number of bytes that <paramref name="name"/> refers to. When this
              function returns, this will point to the number of bytes in the actual name.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Send(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Send a message to another process within the communicator. 
            See <see cref="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Recv(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,MPI.Unsafe.MPI_Status*)">
            <summary>
            Receive a message from another process within the communicator. 
            See <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Test_cancelled(MPI.Unsafe.MPI_Status*,System.Int32*)">
            <summary>
            Determine whether a particular communication operation was cancelled.
            See <see cref="P:MPI.Status.Cancelled"/>.
            </summary>
            <param name="status">Status object</param>
            <param name="flag">Will be set to a non-zero value if the communnication was cancelled.</param>
            <returns>Error code.</returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Get_count(MPI.Unsafe.MPI_Status*,System.Int32,System.Int32*)">
            <summary>
            Determine the number of elements transmitted by a communication operation.
            See <see cref="M:MPI.Status.Count(System.Type)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Isend(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
            An immediate (non-blocking) point-to-point send. 
            See <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>.
            </summary>
            <param name="buf">Buffer of data to send.</param>
            <param name="count">The number of elements in <paramref name="buf"/>.</param>
            <param name="datatype">The type of data in <paramref name="buf"/>.</param>
            <param name="dest">Rank of the destination process.</param>
            <param name="tag">Tag used to transmit this data.</param>
            <param name="comm">Communicator through which this data will be sent.</param>
            <param name="request">Receives a request object that can be used to query this communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Irecv(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
            A non-blocking receive that posts the intent to receive a value. The actual receive will be
            completed when the corresponding request is completed.
            </summary>
            <param name="buf">Buffer that will receive message data.</param>
            <param name="count">Number of elements in <paramref name="buf"/>.</param>
            <param name="datatype">Type of data stored in <paramref name="buf"/>.</param>
            <param name="source">Rank of the processor that will initiate this message, or <see cref="F:MPI.Unsafe.MPI_ANY_SOURCE"/>.</param>
            <param name="tag">Message tag used to identify the message, or <see cref="F:MPI.Unsafe.MPI_ANY_TAG"/>.</param>
            <param name="comm">Communicator through which the message will be sent.</param>
            <param name="request">Receives a request object that can be used to query this communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Wait(System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Wait until the given request has completed. See <see cref="M:MPI.Request.Wait"/>.
            </summary>
            <param name="request">Request object.</param>
            <param name="status">Will receive the status of the completed operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Test(System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Test whether the given request has completed. See <see cref="M:MPI.Request.Test"/>.
            </summary>
            <param name="request">Request object.</param>
            <param name="flag">Will be set to a non-zero value if the request has completed.</param>
            <param name="status">Will receive the status of the completed operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Request_free(System.Int32*)">
            <summary>
            Free the resources associated with a request.
            </summary>
            <param name="request">The request that will be freed.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitany(System.Int32,System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Waits until any of the given MPI requests completes before returning.
            See <see cref="M:MPI.RequestList.WaitAny"/>.
            </summary>
            <param name="count">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">An array of MPI request objects.</param>
            <param name="index">Receives the index of the request that completed.</param>
            <param name="status">Receives the status of the completed request.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testany(System.Int32,System.Int32*,System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Test whether any of the MPI requests has completed.
            See <see cref="M:MPI.RequestList.TestAny"/>.
            </summary>
            <param name="count">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">An array of MPI request objects.</param>
            <param name="index">Receives the index of the request that completed (if <paramref name="flag"/> is non-zero).</param>
            <param name="flag">Will be set to a non-zero value if a request has completed.</param>
            <param name="status">Receives the status of the completed request (if <paramref name="flag"/> is non-zero).</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitall(System.Int32,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Wait until all of the given MPI requests have completed before returning.
            See <see cref="M:MPI.RequestList.WaitAll"/>.
            </summary>
            <param name="count">The number of requests (and statuses).</param>
            <param name="array_of_requests">An array of MPI request objects to be completed.</param>
            <param name="array_of_statuses">An array of MPI status objects, to be filled in by the completed requests.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testall(System.Int32,System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Test whether all of the given MPI requests have been completed.
            See <see cref="M:MPI.RequestList.TestAll"/>.
            </summary>
            <param name="count">The number of requests (and statuses).</param>
            <param name="array_of_requests">An array of MPI request objects to be completed.</param>
            <param name="flag">Will be set to a non-zero value if all requests have completed.</param>
            <param name="array_of_statuses">An array of MPI status objects, to be filled in by the completed requests (if <paramref name="flag"/> is non-zero).</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitsome(System.Int32,System.Int32*,System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Wait until some MPI requests have completed, then provide a list of all of the requests that have completed.
            See <see cref="M:MPI.RequestList.WaitSome"/>.
            </summary>
            <param name="incount">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">The array of requests to be completed.</param>
            <param name="outcount">Receives the number of requests that have been completed.</param>
            <param name="array_of_indices">
              An array that will receive the indices into <paramref name="array_of_requests"/> of the
              completed requests.
            </param>
            <param name="array_of_statuses">
              Array containing the completed status information that corresponds to the completed
              requests whose indices are in <paramref name="array_of_indices"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testsome(System.Int32,System.Int32*,System.Int32*,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Providing a list of all of the requests that have completed, without waiting for any
            requests to complete. See <see cref="M:MPI.RequestList.TestSome"/>.
            </summary>
            <param name="incount">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">The array of requests to be completed.</param>
            <param name="outcount">Receives the number of requests that have been completed.</param>
            <param name="array_of_indices">
              An array that will receive the indices into <paramref name="array_of_requests"/> of the
              completed requests.
            </param>
            <param name="array_of_statuses">
              Array containing the completed status information that corresponds to the completed
              requests whose indices are in <paramref name="array_of_indices"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Iprobe(System.Int32,System.Int32,System.Int32,System.Int32*,MPI.Unsafe.MPI_Status*)">
            <summary>
            Test whether a message is available. See <see cref="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Probe(System.Int32,System.Int32,System.Int32,MPI.Unsafe.MPI_Status*)">
            <summary>
            Wait until a message is available. See <see cref="M:MPI.Communicator.Probe(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cancel(System.Int32*)">
            <summary>
            Cancel an outstanding MPI communication request. See <see cref="M:MPI.Request.Cancel"/>.
            </summary>
            <param name="request">The request to be cancelled.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Wtime">
            <summary>
            Returns a floating point number of seconds, since some time in the past
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allreduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a parallel reduction operation that summarizes the results from the input provided
            by all of the processes in the communicator. Semantically, this is equivalent to an
            <see cref="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)"/> to an arbitrary root followed by an <see cref="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> from 
            that process.
            See <see cref="M:MPI.Communicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">
              Buffer containing the "outgoing" values contributed by the calling process to the reduction operation.
            </param>
            <param name="recvbuf">
              Buffer that will receive the results of the parallel reduction.
            </param>
            <param name="count">
              The number of elements in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="datatype">
              The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="op">
              The MPI reduction operation to use, which may be one of the predefined reduction operations
              or a user-defined operation created with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32*)"/>.
            </param>
            <param name="comm">
              The communicator over which the reduction will occur.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This operation is equivalent to a <see cref="M:MPI.Unsafe.MPI_Gather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> to an arbitrary
            root followed by an <see cref="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> from that root.
            See <see cref="M:MPI.Communicator.Allgather``1(``0)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">Buffer that will contain all of the values contributed by every process.</param>
            <param name="recvcount">The number of elements to receive from each process.</param>
            <param name="recvtype">The type of data that will be stored in the receive buffer.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This operation differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that it 
            permits different processes to provide a different number of elements to be gathered.
            See <see cref="M:MPI.Communicator.Allgather``1(``0)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">Buffer that will contain all of the values contributed by every process.</param>
            <param name="recvcounts">
              An array whose ith element is the number of elements to be received from the process 
              with rank i.
            </param>
            <param name="displs">
              An array whose ith element is the offset (in <paramref name="recbuf"/>) at which the
              data from process i should be placed.
            </param>
            <param name="recvtype">The type of data that will be stored in the receive buffer.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. To send a different amount of data to each process, use 
            <see cref="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)"/> or <see cref="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.Int32)"/>.
            See <see cref="M:MPI.Communicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcount">The number of elements to send to each process.</param>
            <param name="sendtype">The type of data stored in <paramref name="sendbuf"/>.</param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcount">
              The number of elements that will be received from each process.
            </param>
            <param name="recvtype">The type of data stored in <paramref name="recvbuf"/>.</param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. If all of your processes send the same amount of data to each other, use
            the simpler <see cref="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>; if you need the data sent to different processes to
            have different datatypes, use <see cref="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.Int32)"/>.
            See <see cref="M:MPI.Communicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcounts">
              An array whose ith element contains the number of elements to be send to the process with
              rank i.
            </param>
            <param name="sdispls">
              An array whose ith element contains the offsets into <paramref name="sendbuf"/> where the
              data destined for the process with rank i begins.
            </param>
            <param name="sendtype">The type of data in <paramref name="sendbuf"/>.</param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcounts">
              An array whose jth element contains the number of elements that will be received from the
              process with rank j.
            </param>
            <param name="rdispls">
              An array whose jth element contains the offset into <paramref name="recvbuf"/> where the
              data received from the process with rank j begins.
            </param>
            <param name="recvtype">The type of data in <paramref name="recvbuf"/>.</param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.IntPtr,System.Int32*,System.Int32*,System.Int32*,System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. If all of your processes send the same amount of data to each other, use
            the simpler <see cref="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>; if the volume of data sent to each process can be different
            but all of the data has the same type, use <see cref="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32)"/>.
            See <see cref="M:MPI.Communicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcnts">
              An array whose ith element contains the number of elements to be send to the process with
              rank i.
            </param>
            <param name="sdispls">
              An array whose ith element contains the offsets into <paramref name="sendbuf"/> where the
              data destined for the process with rank i begins.
            </param>
            <param name="sendtypes">
              An array whose ith element contains the type of data that will be sent to rank i.
            </param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcnts">
              An array whose jth element contains the number of elements that will be received from the
              process with rank j.
            </param>
            <param name="rdispls">
              An array whose jth element contains the offset into <paramref name="recvbuf"/> where the
              data received from the process with rank j begins.
            </param>
            <param name="recvtypes">
              An array whose jth element contains the type of data that will be received from the process
              with rank j.
            </param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Barrier(System.Int32)">
            <summary>
            A synchronization barrier where no processor leaves the barrier until all processors have entered the barrier.
            See <see cref="M:MPI.Communicator.Barrier"/>.
            </summary>
            <param name="comm">The communicator whose processes will be synchronized.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Broadcast a value from the root process to every process within the communication.
            
            See <see cref="M:MPI.Communicator.Broadcast``1(``0@,System.Int32)"/>.
            </summary>
            <param name="buffer">
              Pointer to the data that will be broadcast. At the root, this buffer will be
              read; in all other processes, the buffer will be written.
            </param>
            <param name="count">The number of elements that <paramref name="buffer"/> points to.</param>
            <param name="datatype">The type of data stored in the <paramref name="buffer"/>.</param>
            <param name="root">The rank of the root processor, from which the data will be broadcast.</param>
            <param name="comm">The communicator over which the data will be transmitted.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Exscan(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Performs a partial exclusive reduction on the data, returning the result from combining the data provided 
            by the first P-1 processes to the process with rank P. 
            See <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">Buffer containing the data to contribute to the reduction.</param>
            <param name="recvbuf">Buffer that will receive the result of combining the first Rank values.</param>
            <param name="count">Number of values in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The reduction operation used to combine values.</param>
            <param name="comm">The communicator over which the communication will occur.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Gather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This routine differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that the results 
            are gathered to only the "root" process, which is identified by its <paramref name="rank"/> 
            in the communicator. 
            See <see cref="M:MPI.Communicator.Gather``1(``0,System.Int32)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">
              Buffer that will contain all of the values contributed by every process.
              This argument is only significant at the root.
            </param>
            <param name="recvcount">
              The number of elements to receive from each process.
              This argument is only significant at the root.
            </param>
            <param name="recvtype">
              The type of data that will be stored in the receive buffer.
              This argument is only significant at the root.
            </param>
            <param name="root">The rank of the "root" process.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Gatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This routine differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that the results 
            are gathered to only the "root" process, which is identified by its <paramref name="rank"/> 
            in the communicator. 
            See <see cref="M:MPI.Communicator.Gather``1(``0,System.Int32)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">
              Buffer that will contain all of the values contributed by every process.
              This argument is only significant at the root.
            </param>
            <param name="recvcounts">
              An array whose ith element is the number of elements to receive from process i.
              This argument is only significant at the root.
            </param>
            <param name="displs">
              An array whose ith element contains the displacement (into <paramref name="recvbuf"/>) at
              which the data from process i will be placed. This argument is only significant at the root.
            </param>
            <param name="recvtype">
              The type of data that will be stored in the receive buffer.
              This argument is only significant at the root.
            </param>
            <param name="root">The rank of the "root" process.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a parallel reduction operation that summarizes the results from the data contributed
            by all of the processes in a communicator. Unlike <see cref="M:MPI.Unsafe.MPI_Allreduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>, the results 
            of this operation are returned only to the process whose rank is equivalent to 
            <paramref name="root"/>, i.e., the "root" process.
            See <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/>
            </summary>
            <param name="sendbuf">
              Buffer containing the "outgoing" values contributed by the calling process to the reduction operation.
            </param>
            <param name="recvbuf">
              Buffer that will receive the results of the parallel reduction. This argument is
              only significant at the root.
            </param>
            <param name="count">
              The number of elements in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="datatype">
              The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="op">
              The MPI reduction operation to use, which may be one of the predefined reduction operations
              or a user-defined operation created with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32*)"/>.
            </param>
            <param name="root">
              Identifies the root process (which will receive the intermediate of the reduction) by
              its rank in the communicator <paramref name="comm"/>.
            </param>
            <param name="comm">
              The communicator over which the reduction will occur.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Reduce_scatter(System.IntPtr,System.IntPtr,System.Int32*,System.Int32,System.Int32,System.Int32)">
            <summary>
            The equivalent of a <see cref="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)"/> followed by a <see cref="M:MPI.Unsafe.MPI_Scatterv(System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>, performing a
            reduction on the data provided in <paramref name="sendbuf"/> and then scattering those results
            to all of the processes. See <see cref="M:MPI.Communicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[])"/>.
            </summary>
            <param name="sendbuf">Buffer containing the data to be reduced.</param>
            <param name="recvbuf">Buffer that will receive this process's results.</param>
            <param name="recvcounts">
              An array whose ith element gives the number of results that will go to the process with rank i.
            </param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The operation used to combine each element in <paramref name="sendbuf"/>.</param>
            <param name="comm">The communicator over which this collective will operate.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scan(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Performs a partial reduction on the data, returning the result from combining the data provided 
            by the first P processes to the process with rank P. 
            See <see cref="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">Buffer containing the data to contribute to the reduction.</param>
            <param name="recvbuf">Buffer that will receive the result of combining the first Rank values.</param>
            <param name="count">Number of values in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The reduction operation used to combine values.</param>
            <param name="comm">The communicator over which the communication will occur.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scatter(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Scatters data from one process (the "root" process) to all of the processes in a communicator,
            with different parts of the data going to different processes. 
            See <see cref="M:MPI.Communicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to be sent. Only significant at the root process.
            </param>
            <param name="sendcount">
              The number of elements to send to each process. Only significant at the root process.
            </param>
            <param name="sendtype">
              The type of data in <paramref name="sendbuf"/>. Only significant at the root process.
            </param>
            <param name="recvbuf">A buffer that will receive the calling process's part of the data.</param>
            <param name="recvcount">The number of elements to receive.</param>
            <param name="rectype">The type of data to receive.</param>
            <param name="root">The rank of the "root" process, which supplies the data.</param>
            <param name="comm">The communicator over which the data will be scattered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scatterv(System.IntPtr,System.Int32*,System.Int32*,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Scatters data from one process (the "root" process) to all of the processes in a communicator,
            with different parts of the data going to different processes. Unlike <see cref="M:MPI.Unsafe.MPI_Scatter(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>,
            different processes may receive different amounts of data.
            See <see cref="M:MPI.Communicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to be sent. Only significant at the root process.
            </param>
            <param name="sendcounts">
              An array whose ith element contains the number of elements to send to process i.
              Only significant at the root process.
            </param>
            <param name="displs">
              An array whose ith element contains the offset (into <paramref name="sendbuf"/>)
              if the data to be sent to process i. Only significant at the root process.
            </param>
            <param name="sendtype">
              The type of data in <paramref name="sendbuf"/>. Only significant at the root process.
            </param>
            <param name="recvbuf">A buffer that will receive the calling process's part of the data.</param>
            <param name="recvcount">The number of elements to receive.</param>
            <param name="recvtype">The type of data to receive.</param>
            <param name="root">The rank of the "root" process, which supplies the data.</param>
            <param name="comm">The communicator over which the data will be scattered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32*)">
            <summary>
            Creates an MPI operation that invokes a user-provided function. The MPI operation
            can be used with various reduction operations. MPI.NET provides support for user-defined
            operations via the <see cref="T:MPI.Operation`1"/> class.
            </summary>
            <param name="function">A pointer to the user-defined function.</param>
            <param name="commute">Whether this function is commutative.</param>
            <param name="op">Receives the newly-created MPI operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Op_free(System.Int32*)">
            <summary>
            Frees an MPI operation created via <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32*)"/>. MPI.NET will automatically
            manage any operations it creates via <see cref="T:MPI.Operation`1"/> when the corresponding
            object is disposed of or finalized.
            </summary>
            <param name="op">The operation to be freed.</param>
        </member>
        <member name="M:MPI.Unsafe.ComparisonFromInt(System.Int32)">
            <summary>
            Converts the intermediate of an MPI_*_compare operation into a <see cref="T:MPI.Comparison"/> enum value.
            </summary>
        </member>
        <member name="T:MPI.Unsafe.MPI_Status">
            <summary>
            Low-level representation of the status of an MPI communication operation.
            Unless you are interacting directly with the low-level MPI interface,
            use <see cref="T:MPI.Status"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.count">
            <summary>
            This field is only available if the underlying MPI is MS-MPI.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.cancelled">
            <summary>
            This field is only available if the underlying MPI is MS-MPI.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_SOURCE">
            <summary>
            The rank of the source of the MPI message.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_TAG">
            <summary>
            The MPI message tag associated with this communication.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_ERROR">
            <summary>
            The MPI error code associated with this communication.
            TODO: We need a higher-level facility for dealing with this.
            </summary>
        </member>
        <member name="T:MPI.Group">
            <summary>
              The <c>Group</c> class provides the ability to manipulate sets of MPI processes.
            </summary>
        </member>
        <member name="F:MPI.Group.NoProcess">
            <summary>
            Sentinel value used to indicate that a particular process is not part of a group.
            </summary>
        </member>
        <member name="F:MPI.Group.empty">
            <summary>
            A predefined group containing no processes.
            </summary>
        </member>
        <member name="M:MPI.Group.#ctor">
            <summary>
              Users cannot construct a Group directly. They typically will retrieve a group from
              a communicator, from other groups, or, in rare cases, adopt one from the low-level
              interface.
            </summary>
        </member>
        <member name="M:MPI.Group.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI group that was created with any of the low-level MPI facilities.
              The resulting <c>Group</c> object will manage the lifetime of the low-level MPI group,
              and will call <see cref="M:MPI.Unsafe.MPI_Group_free(System.Int32*)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI groups through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Group.Finalize">
            <summary>
            Finalizer that frees the MPI group.
            </summary>
        </member>
        <member name="M:MPI.Group.Dispose">
            <summary>
            Free the MPI group explicitly.
            </summary>
        </member>
        <member name="M:MPI.Group.op_BitwiseOr(MPI.Group,MPI.Group)">
            <summary>
            Constructs the union of two groups, containing those processes that are either in
            <paramref name="group1"/> or <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.op_BitwiseAnd(MPI.Group,MPI.Group)">
            <summary>
            Constructs the intersection of two groups, containing those processes that are in
            both <paramref name="group1"/> and <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.op_Subtraction(MPI.Group,MPI.Group)">
            <summary>
            Constructs the difference of two groups, containing those processes that are in
            <paramref name="group1"/> but not <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.IncludeOnly(System.Int32[])">
            <summary>
            Create a subset of this group that includes only the processes with the
            given ranks.
            </summary>
        </member>
        <member name="M:MPI.Group.Exclude(System.Int32[])">
            <summary>
            Create a subset of this group that includes all of the processes in this
            group except those identified by the given ranks.
            </summary>
        </member>
        <member name="M:MPI.Group.TranslateRanks(System.Int32[],MPI.Group)">
            <summary>
              Translates the ranks of processes in this group to the ranks of the same processes within a different group.
            </summary>
            <param name="ranks">The rank values in this group that will be translated.</param>
            <param name="other">The group whose ranks we are translating to.</param>
            <returns>
              An integer array containing the ranks of the processes in <paramref name="other"/> that correspond to
              the ranks of the same processes in this group. For processes that are in this group but not 
              <paramref name="other"/>, the resulting array will contain <see cref="F:MPI.Group.NoProcess"/>.
            </returns>
        </member>
        <member name="M:MPI.Group.Compare(MPI.Group)">
            <summary>
              Compare two MPI groups.
            </summary>
            <list>
              <listheader>
                <term>Value</term>
                <description>Description</description>
              </listheader>
            <item>
              <term><see cref="F:MPI.Comparison.Identical"/></term>
              <description>The two <c>Group</c> objects represent the same group.</description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Congruent"/></term>
              <description>
                The two <c>Group</c> objects contain the same processes with the same ranks,
                but represent different groups.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Similar"/></term>
              <description>
                The two <c>Group</c> objects contain the same processes, but with different ranks.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Unequal"/></term>
              <descsription>The two <c>Group</c> objects are different.</descsription>
            </item>
            </list>      
        </member>
        <member name="F:MPI.Group.group">
            <summary>
            The low-level MPI group handle.
            </summary>
        </member>
        <member name="P:MPI.Group.Size">
            <summary>
              The number of processes within this group.
            </summary>
        </member>
        <member name="P:MPI.Group.Rank">
            <summary>
              The rank of the calling process within this group. This will be a value in [0, <see cref="P:MPI.Group.Size"/>-1).
            </summary>
        </member>
        <member name="T:MPI.RequestList">
            <summary>
            A request list contains a list of outstanding MPI requests. These requests
            are typically non-blocking send or receive operations (e.g.,
            <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>,
            <see cref="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32)"/>). The
            request list provides the ability to operate on the set of MPI requests
            as a whole, for example by waiting until all requests complete before
            returning or testing whether any of the requests have completed.
            </summary>
        </member>
        <member name="M:MPI.RequestList.#ctor">
            <summary>
            Create a new, empty request list.
            </summary>
        </member>
        <member name="M:MPI.RequestList.Add(MPI.Request)">
            <summary>
            Add a new request to the request list.
            </summary>
            <param name="request">The request to add.</param>
        </member>
        <member name="M:MPI.RequestList.Remove(MPI.Request)">
            <summary>
            Remove a request from the request list.
            </summary>
            <param name="request">Request to remove.</param>
        </member>
        <member name="M:MPI.RequestList.WaitAny">
            <summary>
            Waits until any request has completed. That request will then be removed 
            from the request list and returned.
            </summary>
            <returns>The completed request, which has been removed from the request list.</returns>
        </member>
        <member name="M:MPI.RequestList.TestAny">
            <summary>
            Determines whether any request has completed. If so, that request will be removed
            from the request list and returned. 
            </summary>
            <returns>
              The first request that has completed, if any. Otherwise, returns <c>null</c> to
              indicate that no request has completed.
            </returns>
        </member>
        <member name="M:MPI.RequestList.WaitAll">
            <summary>
            Wait until all of the requests has completed before returning.
            </summary>
            <returns>A list containing all of the completed requests.</returns>
        </member>
        <member name="M:MPI.RequestList.TestAll">
            <summary>
            Test whether all of the requests have completed. If all of the
            requests have completed, the result is the list of requests. 
            Otherwise, the result is <c>null</c>.
            </summary>
            <returns>Either the list of all completed requests, or null.</returns>
        </member>
        <member name="M:MPI.RequestList.WaitSome">
            <summary>
            Wait for at least one request to complete, then return a list of
            all of the requests that have completed at this point.
            </summary>
            <returns>
              A list of all of the requests that have completed, which
              will contain at least one element.
            </returns>
        </member>
        <member name="M:MPI.RequestList.TestSome">
            <summary>
            Return a list of all requests that have completed.
            </summary>
            <returns>
              A list of all of the requests that have completed. If
              no requests have completed, returns <c>null</c>.
            </returns>
        </member>
        <member name="F:MPI.RequestList.requests">
            <summary>
            The actual list of requests.
            </summary>
        </member>
        <member name="T:MPI.Request">
            <summary>
            A non-blocking communication request. Each request object refers to a single
            communication operation, such as non-blocking send 
            (see <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>)
            or receive. Non-blocking operations may progress in the background, and can complete
            without any user intervention. However, it is crucial that outstanding communication
            requests be completed with a successful call to <see cref="M:MPI.Request.Wait"/> or <see cref="M:MPI.Request.Test"/>
            before the request object is lost.
            </summary>
        </member>
        <member name="M:MPI.Request.Wait">
            <summary>
            Wait until this non-blocking operation has completed.
            </summary>
            <returns>
              Information about the completed communication operation.
            </returns>
        </member>
        <member name="M:MPI.Request.Test">
            <summary>
            Determine whether this non-blocking operation has completed.
            </summary>
            <returns>
            If the non-blocking operation has completed, returns information
            about the completed communication operation. Otherwise, returns
            <c>null</c> to indicate that the operation has not completed.
            </returns>
        </member>
        <member name="M:MPI.Request.Cancel">
            <summary>
            Cancel this communication request.
            </summary>
        </member>
        <member name="T:MPI.ReceiveRequest">
            <summary>
            A non-blocking receive request. This class allows one to test a receive
            request for completion, wait for completion of a request, cancel a request,
            or extract the value received by this communication request.
            </summary>
        </member>
        <member name="M:MPI.ReceiveRequest.GetValue">
            <summary>
            Retrieve the value received via this communication. The value
            will only be available when the communication has completed.
            </summary>
            <returns>The value received by this communication.</returns>
        </member>
        <member name="T:MPI.ValueTypeSendRequest">
            <summary>
            A request that corresponds to a non-blocking send of either a 
            single value or an array of values, where the type of the value
            (or values) is a value type with an associated MPI datatype.
            Underneath, this request is a single <c>MPI_Request</c> object
            coupled with a handle to pinned memory and some additional 
            information about the send itself.
            </summary>
        </member>
        <member name="M:MPI.ValueTypeSendRequest.#ctor(System.Int32,System.Int32,System.Runtime.InteropServices.GCHandle)">
            <summary>
            Initialize a single request for a non-blocking send that has already been initiated.
            </summary>
            <param name="request">The request object.</param>
            <param name="count">The number of elements transmitted in this communication.</param>
            <param name="handle">A handle pointing to pinned memory that will be unpinned after
            this operation completes.</param>
        </member>
        <member name="M:MPI.ValueTypeSendRequest.Cleanup">
            <summary>
            Cleanup any resources that we're currently holding. This typically
            involves unpinning the memory associated with this request.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.request">
            <summary>
            The underlying MPI request object that provides a handle to this
            non-blocking send.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.count">
            <summary>
            The number of elements sent by this operation. 
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.handle">
            <summary>
            Handle to the pinned memory used in this request.
            </summary>
        </member>
        <member name="T:MPI.TwoRequests">
            <summary>
            A structure storing two requests. Use this to keep the two requests in memory
            next to each other, so that we can complete or test them with a single MPI call.
            </summary>
        </member>
        <member name="F:MPI.TwoRequests.data">
            <summary>
            The two requests.
            </summary>
        </member>
        <member name="T:MPI.SerializedSendRequest">
            <summary>
            A request for a serialized send, which typically consists of two <c>MPI_Request</c> objects.
            This class manages pinning and unpinning the data for both send requests, and testing/completing
            both requests in the same operation.
            </summary>
        </member>
        <member name="M:MPI.SerializedSendRequest.#ctor(MPI.Communicator,System.Int32,System.Int32,System.Byte[],System.Int32)">
            <summary>
            Initiate an MPI non-blocking send operation managed by this Request object.
            </summary>
            <param name="comm">The communicator over which the initial message will be sent.</param>
            <param name="dest">The destination rank for this message.</param>
            <param name="tag">The message tag.</param>
            <param name="buffer">A buffer containing the serialized data to be sent.</param>
            <param name="count">The number of serialized objects stored in <paramref name="buffer"/>.</param>
        </member>
        <member name="M:MPI.SerializedSendRequest.Cleanup">
            <summary>
            Clean up whatever resources we are holding.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.count">
            <summary>
            The number of elements transmitted by this operation. 
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.headerObj">
            <summary>
            Header sent in the first message corresponding to the serialized send.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.tagAllocator">
            <summary>
            Tag allocator user by the communicator to temporarily retrieve tags
            for sending serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.headerHandle">
            <summary>
            Handle that pins down the header object, so that it does not move.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.bufferHandle">
            <summary>
            Handle pinning the address of the buffer.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.requests">
            <summary>
            The two outstanding MPI requests. The first request contains the send request
            for the buffer, while the second request contains the send for the length
            (which may be <see cref="F:MPI.Unsafe.MPI_REQUEST_NULL"/>).
            </summary>
        </member>
        <member name="T:MPI.ValueReceiveRequest`1">
            <summary>
            A non-blocking receive request for a single value of value type. This
            request contains only a single <c>MPI_Request</c> object, which will
            receive into its own <c>value</c> member.
            </summary>
        </member>
        <member name="M:MPI.ValueReceiveRequest`1.Cleanup">
            <summary>
            Cleanup any resources we're still holding on to.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.handle">
            <summary>
            Handle to <c>this</c>, which will be pinned so that <c>value</c>'s
            address will remain the same.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.request">
            <summary>
            The MPI request associated with the non-blocking receive.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.value">
            <summary>
            The actual value we'll be receiving
            </summary>
        </member>
        <member name="T:MPI.ValueArrayReceiveRequest`1">
            <summary>
            A non-blocking receive request for an array of values of value type. This
            request contains only a single <c>MPI_Request</c> object, which will
            receive into the given array.
            </summary>
        </member>
        <member name="M:MPI.ValueArrayReceiveRequest`1.Cleanup">
            <summary>
            Cleanup any resources we're still holding on to.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.handle">
            <summary>
            Handle to <c>this</c>, which will be pinned so that <c>value</c>'s
            address will remain the same.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.request">
            <summary>
            The MPI request associated with the non-blocking receive.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.array">
            <summary>
            The actual array we'll be receiving into.
            </summary>
        </member>
        <member name="T:MPI.SerializedReceiveRequest`1">
            <summary>
            A request for a serialized receive, which uses a two-stage receive process. The first stage
            receives the size of the serialized data. The second stage receives the actual data and, 
            upon completion, deserializes that data.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.SerializedReceiveRequest`1.SetupSerializedReceive(MPI.Unsafe.MPI_Status)">
            <summary>
            Given the header data, this routine will set up the receive of the
            serialized data on the shadow communicator.
            </summary>
            <param name="status">
              The status message returned from the completion of the first
              receive (of the header data).
            </param>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.handle">
            <summary>
            Handle that pins down either the header object (when receiving the header)
            or the stream's buffer (when receiving the serialized data).
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.request">
            <summary>
            Request object that corresponds to the currently-active request.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.shadowComm">
            <summary>
            "Shadow" communicator through which we will receive the serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.stream">
            <summary>
            Stream that will receive the serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.headerObj">
            <summary>
            Message header to be received in the first stage.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.value">
            <summary>
            The value we are receiving.
            </summary>
        </member>
        <member name="T:MPI.SerializedArrayReceiveRequest`1">
            <summary>
            A request for a serialized receive of an array of values. The communication behavior is 
            identical to (and implemented as) <see cref="T:MPI.SerializedReceiveRequest`1"/>, but
            here we need to copy the values from the de-serialized array into the array provided 
            by the user.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.SerializedArrayReceiveRequest`1.CopyResults(MPI.CompletedStatus)">
            <summary>
            Copies the results from the received array into the user-provided array
            and returns a new status object with the appropriate count.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.request">
            <summary>
            The request that handles receipt of the actual array, since we cannot
            de-serialize an array into an existing array.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.array">
            <summary>
            The user-provided array, where values will be copied once the receive
            has completed.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.cachedStatus">
            <summary>
            Cached copy of the status object returned from this request. When non-null,
            this communication has already completed, so we just return this object.
            </summary>
        </member>
        <member name="T:MPI.Intracommunicator">
            <summary>
              Intracommunicators are the most commonly used form of communicator in MPI. 
              Each intracommunicator contains a set of processes, each of which is identified by its
              "rank" within the communicator. The ranks are numbered 0 through Size-1. 
              Any process in the communicator can send a message to another process
              within the communicator or receive a message from any other process in 
              the communicator. Intracommunicators also support a variety of collective operations
              that involve all of the processes in the communicator. Most MPI communication occurs
              within intracommunicators, with very few MPI programs requiring intercommunicators.
            </summary>
        </member>
        <member name="M:MPI.Intracommunicator.#ctor">
            <summary>
            Intracommunicators can only be constructed from other communicators or adopted
            from low-level intracommunicators.
            </summary>
        </member>
        <member name="M:MPI.Intracommunicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI intercommunicator that was created with any of the low-level MPI facilities.
              The resulting <c>Intercommunicator</c> object will manage the lifetime of the low-level MPI intercommunicator,
              and will call <see cref="M:MPI.Unsafe.MPI_Comm_free(System.Int32*)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI intercommunicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the values stored by each process into 
              partial results delivered to each process. The values are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>value1 op value2 op ... op valueP</c>. The rank 0 process receives a default value.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by the calling process.</param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <returns>
              If the rank of the calling process is not zero, the value <c>value1 op value2 op ... op valueP</c>, where P 
              is the rank of the calling process. Otherwise, returns a default value.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the arrays stored by each process into 
              partial results delivered to each process. The values are combined in a user-defined way, specified via 
              a delegate that will be applied elementwise to the values in the arrays. If <c>array1</c>, <c>array2</c>, ..., <c>arrayN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>array1 op array2 op ... op arrayP</c>. The rank 0 process receives a <c>null</c> array.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The values contributed by the calling process. The lengths of the arrays provided by all of the
              processes must be the same.
            </param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <returns>
              If the rank of the calling process is not zero, the value <c>array1 op array2 op ... op arrayP</c>, where P 
              is the rank of the calling process. Otherwise, returns a <c>null</c> array.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the arrays stored by each process into 
              partial results delivered to each process. The values are combined in a user-defined way, specified via 
              a delegate that will be applied elementwise to the values in the arrays. If <c>array1</c>, <c>array2</c>, ..., <c>arrayN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>array1 op array2 op ... op arrayP</c>. The rank 0 process receives a <c>null</c> array.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values contributed by the calling process. The lengths of the arrays provided by all of the
              processes must be the same.
            </param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <param name="outValues">
              If the rank of the calling process is not zero, this will receive the value <c>array1 op array2 op ... op arrayP</c>, where P 
              is the rank of the calling process. Otherwise, this value will be replaced with <c>null</c>.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the values stored by each process into 
              partial results delivered to each process. The values are combined 
              in a user-defined way, specified via a delegate. If <c>value(0)</c>, <c>value(1)</c>, ..., <c>value(N-1)</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P will be <c>value(0) op value(1) op ... op value(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a <see cref="M:MPI.Communicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by the calling process.</param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <returns>The value <c>value(0) op value(1) op ... op value(Rank)</c></returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the arrays stored by each process into 
              partial results delivered to each process. The arrays are combined in a user-defined way, 
              specified via a delegate that will be applied elementwise to the values in each arrays. 
              If <c>array(0)</c>, <c>array(1)</c>, ..., <c>array(N-1)</c> are the arrays provided by the 
              N processes in the communicator, the resulting array for the process with 
              rank P will be <c>array(0) op array(1) op ... op array(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a 
              <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0})"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array contributed by the calling process. The arrays provided by each process must
               have the same length.
            </param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <returns>The array <c>array(0) op array(1) op ... op array(Rank)</c></returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the arrays stored by each process into 
              partial results delivered to each process. The arrays are combined in a user-defined way, 
              specified via a delegate that will be applied elementwise to the values in each arrays. 
              If <c>array(0)</c>, <c>array(1)</c>, ..., <c>array(N-1)</c> are the arrays provided by the 
              N processes in the communicator, the resulting array for the process with 
              rank P will be <c>array(0) op array(1) op ... op array(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a 
              <see cref="M:MPI.Communicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array contributed by the calling process. The arrays provided by each process must
               have the same length.
            </param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <param name="outValues">The array <c>array(0) op array(1) op ... op array(Rank)</c></param>
        </member>
        <member name="T:MPI.Threading">
            <summary>
            Enumeration describing the level of threading support provided by the MPI implementation.
            The MPI environment should be initialized with the minimum threading support required
            for your application, because additional threading support can have a negative impact
            on performance.
            
            The four options providing monotonically-increasing levels of freedom for the MPI
            program. Thus, a program implemented based on the <c>Threading.Single</c> semantics
            will work perfectly well (although perhaps less efficiently) with <c>Threading.Multiple</c>.
            </summary>
        </member>
        <member name="F:MPI.Threading.Single">
            <summary>
            The program is single-threaded, and does not ever create additional threads
            (even those that don't use MPI).
            </summary>
        </member>
        <member name="F:MPI.Threading.Funneled">
            <summary>
            The program is multi-threaded, but only one of those threads (the main thread)
            will make calls into the MPI library. Thus, all of the MPI calls are "funneled"
            into that one thread. One can determine if the currently-executing thread is
            the main thread via <see cref="P:MPI.Environment.IsMainThread"/>.
            </summary>
        </member>
        <member name="F:MPI.Threading.Serialized">
            <summary>
            The program is multi-threaded, and more than one thread will make calls into 
            the MPI library. However, these calls into MPI are "serialized", in that
            no two threads will call the MPI library at the same time. This mode essentially
            requires the user program to have the equivalent of a global lock around all calls
            to MPI.
            </summary>
        </member>
        <member name="F:MPI.Threading.Multiple">
            <summary>
            The program is multi-threaded, and more than one thread will make calls into
            the MPI library. The program is free to call the MPI library routines from
            any thread at any time, including having multiple threads calling MPI
            concurrently. Note that this level of threading is not supported by MS MPI.
            </summary>
        </member>
        <member name="T:MPI.Environment">
            <summary>Provides MPI initialization, finalization, and environmental queries.</summary>
            <remarks>
              The <c>Environment</c> class provides initialization, finalization, and
              environmental query operations for the Message Passing Interface.
              Users must initialize the MPI environment before using any of the
              MPI facilities by creating an instanceof the Environment class.
            </remarks>
        </member>
        <member name="M:MPI.Environment.#ctor(System.String[]@)">
            <summary>
              Initializes the MPI environment. This variant of the <see cref="T:MPI.Environment"/> constructor 
              initializes the MPI environment with the <see cref="F:MPI.Threading.Serialized"/> threading model.
            </summary>
            <param name="args">
              Arguments passed to the <c>Main</c> function in your program. MPI 
              may use some of these arguments for its initialization, and will remove 
              them from this argument before returning.
            </param>
            <remarks>
              This routine must be invoked before using any other MPI facilities. 
              Be sure to call <c>Dispose()</c> to finalize the MPI environment before exiting!
            </remarks>
            <example>This simple program initializes MPI and writes out the rank of each processor:
            <code>
            using MPI;
            
            public class Hello 
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        System.Console.WriteLine("Hello, from process number " 
                            + MPI.Communicator.world.Rank.ToString() + " of "
                            + MPI.Communicator.world.Size.ToString());
                    }
                }
            }
            </code>
            </example>
        </member>
        <member name="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)">
            <summary>Initializes the MPI environment.</summary>
            <param name="args">
              Arguments passed to the <c>Main</c> function in your program. MPI 
              may use some of these arguments for its initialization, and will remove 
              them from this argument before returning.
            </param>
            <param name="threading">
              The level of threading support requested of the MPI implementation. The
              implementation will attempt to provide this level of threading support.
              However, the actual level of threading support provided will be published
              via the <see cref="P:MPI.Environment.Threading"/> property.
            </param>
            <remarks>
              This routine must be invoked before using any other MPI facilities. 
              Be sure to call <c>Dispose()</c> to finalize the MPI environment before exiting!
            </remarks>
            <example>This simple program initializes MPI and writes out the rank of each processor:
            <code>
            using MPI;
            
            public class Hello 
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        System.Console.WriteLine("Hello, from process number " 
                            + MPI.Communicator.world.Rank.ToString() + " of "
                            + MPI.Communicator.world.Size.ToString());
                    }
                }
            }
            </code>
            </example>
        </member>
        <member name="M:MPI.Environment.Dispose">
            <summary>
             Finalizes the MPI environment. Users must call this routine to shut down MPI.
            </summary>
        </member>
        <member name="M:MPI.Environment.Finalize">
            <summary>
              Verifies that the MPI environment has been finalized by calling <c>Dispose()</c>.
            </summary>
        </member>
        <member name="M:MPI.Environment.Abort(System.Int32)">
            <summary>
            Terminates all processes.
            </summary>
            <param name="errorcode">An error code that will be returned to the invoking environment.</param>
        </member>
        <member name="P:MPI.Environment.Initialized">
            <summary>
            Determine whether the MPI environment has been initialized.
            </summary>
        </member>
        <member name="P:MPI.Environment.Finalized">
            <summary>
            Determine whether the MPI environment has been finalized.
            </summary>
        </member>
        <member name="P:MPI.Environment.Threading">
            <summary>
            The level of threading support provided by the MPI library.
            This value describes whether and how the MPI library can be
            used in multi-threaded programs. The threading level is
            requested when the MPI library is initialized in the
            <see cref="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)"/>
            constructor.
            </summary>
        </member>
        <member name="P:MPI.Environment.IsMainThread">
            <summary>
            Determines whether the calling thread is the main MPI thread. Will return
            true for the thread that called the <see cref="T:MPI.Environment"/> constructor
            to initialize MPI. The main thread is particularly important when the threading
            mode is <see cref="F:MPI.Threading.Funneled"/>, because in that model only the
            main thread can invoke MPI routines.
            </summary>
        </member>
        <member name="P:MPI.Environment.ProcessorName">
            <summary>
            Returns the name of the currently executing processor.
            This name does not have any specific form, but typically
            identifies the computere on which the proces is executing.
            </summary>
        </member>
        <member name="P:MPI.Environment.Wtime">
            <summary>
            Returns the time, in seconds, since some arbitrary time in the past.
            This value is typically used for timing parallel applications.
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache">
            <summary>
            Provides a mapping from .NET types to their corresponding MPI datatypes.
            This class should only be used by experts in both MPI's low-level (C) 
            interfaces and the interaction between managed and unmanaged code in .NET.
            </summary>
        </member>
        <member name="M:MPI.DatatypeCache.BuildDatatype(System.Type)">
            <summary>
            Builds a new MPI datatype by using reflection on the given type.
            </summary>
        </member>
        <member name="M:MPI.DatatypeCache.GetDatatype(System.Type)">
            <summary>
            Get the MPI datatype associated with the given type. 
            </summary>
            <returns>
              <see cref="F:MPI.Unsafe.MPI_DATATYPE_NULL"/> if there is no corresponding MPI 
              datatype for this type. Otherwise, returns the MPI datatype that can be
              used to transfer objects of this type without serialization.
            </returns>
        </member>
        <member name="M:MPI.DatatypeCache.BuiltinDatatypes">
            <summary>
            Builds a dictionary containing all of the built-in datatypes.
            </summary>
            <returns></returns>
        </member>
        <member name="F:MPI.DatatypeCache.cache">
            <summary>
            Contains a mapping from value types to their MPI datatype equivalents
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache.Packed">
            <summary>
            Placeholder type that is used to indicate that data being sent by one of the
            low-level MPI routines is packed by <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32*,System.Int32)"/> and will be
            unpacked by <see cref="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32*,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache.LowerBound">
            <summary>
            Placeholder type that is used to mark the lower bound of an MPI derived data type.
            This placeholder type should only be used by MPI experts that require the flexibility
            of the lowest-level MPI for constructing special MPI datatypes. This type is mapped to
            <see cref="F:MPI.Unsafe.MPI_LB"/>.
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache.UpperBound">
            <summary>
            Placeholder type that is used to mark the upper bound of an MPI derived data type.
            This placeholder type should only be used by MPI experts that require the flexibility
            of the lowest-level MPI for constructing special MPI datatypes. This type is mapped to
            <see cref="F:MPI.Unsafe.MPI_UB"/>.
            </summary>
        </member>
    </members>
</doc>
